{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Initial metadeta formation and cleaning the file entries....assigning certail datatypes to certain columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrDtpS38z654"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.metrics import accuracy_score, classification_report, mean_absolute_error, r2_score\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Paths\n",
        "training_info_path = '/content/drive/MyDrive/IMES/imes/Case_Information_Training.xlsx'\n",
        "testing_info_path = '/content/drive/MyDrive/IMES/imes/Case_Information_Testing.xlsx'\n",
        "training_data_path = '/content/drive/MyDrive/IMES/imes/data_files/Raw Data/Training Data'\n",
        "testing_data_path = '/content/drive/MyDrive/IMES/imes/data_files/Raw Data/Testing Data'\n",
        "output_base_path = '/content/drive/MyDrive/IMES/imes/processed'\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(output_base_path, exist_ok=True)\n",
        "\n",
        "df_training_raw = pd.read_excel(training_info_path, engine='openpyxl')\n",
        "df_testing_raw = pd.read_excel(testing_info_path, engine='openpyxl')\n",
        "\n",
        "def clean_metadata(df):\n",
        "    df_clean = df.copy()\n",
        "    df_clean.columns = df_clean.columns.str.strip()\n",
        "    df_clean = df_clean.replace(['None', 'none', ''], np.nan)\n",
        "\n",
        "    binary_cols = ['Defect', 'Frequency Defect', 'Inclination Defect', 'Damper Defect', 'Belt Speed Defect']\n",
        "    for col in binary_cols:\n",
        "        if col in df_clean.columns:\n",
        "            df_clean[col] = df_clean[col].map({'Yes': 1, 'No': 0}).astype('Int64')\n",
        "\n",
        "    if 'Sensor Position' in df_clean.columns:\n",
        "        df_clean['Sensor Position'] = df_clean['Sensor Position'].astype('int8')\n",
        "    if 'File Name' in df_clean.columns:\n",
        "        df_clean['File Name'] = df_clean['File Name'].astype('string').str.strip()\n",
        "\n",
        "    return df_clean\n",
        "\n",
        "def fix_training_case_numbers(df):\n",
        "    perfect_mask = df['File Name'].str.contains('perfect')\n",
        "    df.loc[perfect_mask, 'Case_ID'] = '0_' + df.loc[perfect_mask, 'Sensor Position'].astype(str)\n",
        "    df.loc[perfect_mask, 'File Name'] = 'G0_P' + df.loc[perfect_mask, 'Sensor Position'].astype(str) + '_case_perfect.xls'\n",
        "\n",
        "    non_perfect_mask = ~perfect_mask\n",
        "    case_nums = df.loc[non_perfect_mask, 'File Name'].str.extract(r'case(\\d+)', expand=False).astype('Int64')\n",
        "    group_digit = case_nums // 10\n",
        "    case_digit = case_nums % 10\n",
        "    df.loc[non_perfect_mask, 'Case_ID'] = group_digit.astype(str) + '_' + case_digit.astype(str)\n",
        "\n",
        "    return df\n",
        "\n",
        "def fix_testing_case_numbers(df):\n",
        "    df['Case_ID'] = df['Case No.'].astype(str)\n",
        "    return df\n",
        "\n",
        "def check_file_existence(df, data_path):\n",
        "    existing_files = []\n",
        "    missing_files = []\n",
        "    for _, row in df.iterrows():\n",
        "        file_path = os.path.join(data_path, row['File Name'])\n",
        "        if os.path.exists(file_path):\n",
        "            existing_files.append(row['File Name'])\n",
        "        else:\n",
        "            missing_files.append(row['File Name'])\n",
        "    return existing_files, missing_files\n",
        "\n",
        "df_training_clean = clean_metadata(df_training_raw)\n",
        "df_testing_clean = clean_metadata(df_testing_raw)\n",
        "\n",
        "df_training_clean = fix_training_case_numbers(df_training_clean)\n",
        "df_testing_clean = fix_testing_case_numbers(df_testing_clean)\n",
        "\n",
        "if 'Case No.' in df_training_clean.columns:\n",
        "    df_training_clean = df_training_clean.drop(columns=['Case No.'])\n",
        "if 'Case No.' in df_testing_clean.columns:\n",
        "    df_testing_clean = df_testing_clean.drop(columns=['Case No.'])\n",
        "\n",
        "cols_train = list(df_training_clean.columns)\n",
        "if 'Case_ID' in cols_train:\n",
        "    cols_train.insert(0, cols_train.pop(cols_train.index('Case_ID')))\n",
        "    df_training_clean = df_training_clean[cols_train]\n",
        "\n",
        "cols_test = list(df_testing_clean.columns)\n",
        "if 'Case_ID' in cols_test:\n",
        "    cols_test.insert(0, cols_test.pop(cols_test.index('Case_ID')))\n",
        "    df_testing_clean = df_testing_clean[cols_test]\n",
        "\n",
        "check_file_existence(df_training_clean, training_data_path)\n",
        "check_file_existence(df_testing_clean, testing_data_path)\n",
        "\n",
        "df_combined = pd.concat([\n",
        "    df_training_clean.assign(Dataset='Training'),\n",
        "    df_testing_clean.assign(Dataset='Testing')\n",
        "], ignore_index=True)\n",
        "\n",
        "# Save all files and confirm\n",
        "training_output_path = os.path.join(output_base_path, 'Cleaned_Training_Info.csv')\n",
        "testing_output_path = os.path.join(output_base_path, 'Cleaned_Testing_Info.csv')\n",
        "combined_output_path = os.path.join(output_base_path, 'Combined_Metadata.csv')\n",
        "\n",
        "df_training_clean.to_csv(training_output_path, index=False)\n",
        "df_testing_clean.to_csv(testing_output_path, index=False)\n",
        "df_combined.to_csv(combined_output_path, index=False)\n",
        "\n",
        "print(\"Metadata files saved.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FFT AND FREQUENCY DOMAIN FEATURE EXTRACTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHfG4maG0LCW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from scipy.fft import fft, fftfreq\n",
        "from scipy.signal import butter, filtfilt\n",
        "\n",
        "# Paths\n",
        "training_data_path = '/content/drive/MyDrive/IMES/imes/data_files/Raw Data/Training Data'\n",
        "testing_data_path = '/content/drive/MyDrive/IMES/imes/data_files/Raw Data/Testing Data'\n",
        "output_base_path = '/content/drive/MyDrive/IMES/imes/processed'\n",
        "\n",
        "# Load cleaned metadata\n",
        "training_metadata_path = os.path.join(output_base_path, 'Cleaned_Training_Info.csv')\n",
        "testing_metadata_path = os.path.join(output_base_path, 'Cleaned_Testing_Info.csv')\n",
        "\n",
        "df_training_clean = pd.read_csv(training_metadata_path)\n",
        "df_testing_clean = pd.read_csv(testing_metadata_path)\n",
        "\n",
        "def simple_bandpass_filter(data, fs, lowcut=25.0, highcut=65.0, order=4):\n",
        "    nyquist = 0.5 * fs\n",
        "    low = lowcut / nyquist\n",
        "    high = highcut / nyquist\n",
        "    if high >= 1.0:\n",
        "        high = 0.99\n",
        "    b, a = butter(order, [low, high], btype='band')\n",
        "    y = filtfilt(b, a, data)\n",
        "    return y\n",
        "\n",
        "def detect_peak_frequency(signal, fs, freq_min=25, freq_max=65):\n",
        "    N = len(signal)\n",
        "    yf = fft(signal)\n",
        "    xf = fftfreq(N, 1 / fs)\n",
        "    pos_mask = xf >= 0\n",
        "    xf = xf[pos_mask]\n",
        "    yf = np.abs(yf[pos_mask])\n",
        "    freq_mask = (xf >= freq_min) & (xf <= freq_max)\n",
        "    xf = xf[freq_mask]\n",
        "    yf = yf[freq_mask]\n",
        "    if len(yf) == 0:\n",
        "        return None, None, None\n",
        "    peak_idx = np.argmax(yf)\n",
        "    peak_freq = xf[peak_idx]\n",
        "    peak_mag = yf[peak_idx]\n",
        "    mean_mag = np.mean(yf)\n",
        "    return peak_freq, peak_mag, mean_mag\n",
        "\n",
        "def analyze_file_frequencies_with_sampling(df_metadata, data_path, dataset_name):\n",
        "    results = []\n",
        "    for _, row in df_metadata.iterrows():\n",
        "        file_path = os.path.join(data_path, row['File Name'])\n",
        "        if os.path.exists(file_path):\n",
        "            try:\n",
        "                # Load file and calculate sampling frequency from time data\n",
        "                df = pd.read_excel(file_path, engine='xlrd')\n",
        "                time_data = df.iloc[:, 0].values\n",
        "\n",
        "                # Calculate actual sampling frequency from time intervals\n",
        "                if len(time_data) < 2:\n",
        "                    fs = 100.0\n",
        "                else:\n",
        "                    avg_interval = np.mean(np.diff(time_data))\n",
        "                    fs = round(1.0 / avg_interval, 2) if avg_interval > 0 else 100.0\n",
        "\n",
        "                # FFT analysis with calculated sampling frequency\n",
        "                df.columns = ['Time', 'ax', 'ay', 'az', 'a_abs']\n",
        "                freq_results = {}\n",
        "\n",
        "                for axis in ['ax', 'ay', 'az']:\n",
        "                    filtered_signal = simple_bandpass_filter(df[axis].values, fs)\n",
        "                    peak_freq, peak_mag, mean_mag = detect_peak_frequency(filtered_signal, fs)\n",
        "                    freq_results[f'{axis}_filtered_peak_freq'] = peak_freq\n",
        "                    freq_results[f'{axis}_filtered_peak_mag'] = peak_mag\n",
        "                    freq_results[f'{axis}_filtered_mean_mag'] = mean_mag\n",
        "\n",
        "                result = {\n",
        "                    'File Name': row['File Name'],\n",
        "                    'Dataset': dataset_name,\n",
        "                    'Case_ID': row['Case_ID'],\n",
        "                    'Sensor Position': row['Sensor Position'],\n",
        "                    'Sampling Frequency Hz': fs,\n",
        "                    'Ground_Truth_Freq': row.get('Frequency Value', None),\n",
        "                    'Ground_Truth_Freq_Location': row.get('Frequency Location', None),\n",
        "                    'Has_Frequency_Defect': row.get('Frequency Defect', 0)\n",
        "                }\n",
        "                result.update(freq_results)\n",
        "                results.append(result)\n",
        "\n",
        "            except Exception:\n",
        "                # Add error result with None values\n",
        "                result = {\n",
        "                    'File Name': row['File Name'],\n",
        "                    'Dataset': dataset_name,\n",
        "                    'Case_ID': row['Case_ID'],\n",
        "                    'Sensor Position': row['Sensor Position'],\n",
        "                    'Sampling Frequency Hz': None,\n",
        "                    'Ground_Truth_Freq': row.get('Frequency Value', None),\n",
        "                    'Ground_Truth_Freq_Location': row.get('Frequency Location', None),\n",
        "                    'Has_Frequency_Defect': row.get('Frequency Defect', 0)\n",
        "                }\n",
        "                for axis in ['ax', 'ay', 'az']:\n",
        "                    result[f'{axis}_filtered_peak_freq'] = None\n",
        "                    result[f'{axis}_filtered_peak_mag'] = None\n",
        "                    result[f'{axis}_filtered_mean_mag'] = None\n",
        "                results.append(result)\n",
        "    return results\n",
        "\n",
        "# Perform frequency analysis for training and testing\n",
        "training_results = analyze_file_frequencies_with_sampling(df_training_clean, training_data_path, 'Training')\n",
        "testing_results = analyze_file_frequencies_with_sampling(df_testing_clean, testing_data_path, 'Testing')\n",
        "\n",
        "# Combine all results\n",
        "all_results = training_results + testing_results\n",
        "\n",
        "# Create DataFrame and save\n",
        "frequency_df = pd.DataFrame(all_results)\n",
        "frequency_output_path = os.path.join(output_base_path, 'Frequency_Analysis_Simple.csv')\n",
        "frequency_df.to_csv(frequency_output_path, index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bmEZQHv0h9V"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.fft import fft, fftfreq\n",
        "from scipy.signal import butter, filtfilt\n",
        "import os\n",
        "\n",
        "# Frequency analysis input/output paths and loading\n",
        "output_base_path = '/content/drive/MyDrive/IMES/imes/processed'\n",
        "training_data_path = '/content/drive/MyDrive/IMES/imes/data_files/Raw Data/Training Data'\n",
        "testing_data_path = '/content/drive/MyDrive/IMES/imes/data_files/Raw Data/Testing Data'\n",
        "\n",
        "frequency_df = pd.read_csv(os.path.join(output_base_path, 'Frequency_Analysis_Simple.csv'))\n",
        "\n",
        "training_data = frequency_df[frequency_df['Dataset'] == 'Training'].copy()\n",
        "testing_data = frequency_df[frequency_df['Dataset'] == 'Testing'].copy()\n",
        "\n",
        "feature_cols = [\n",
        "    'ax_filtered_peak_freq', 'ay_filtered_peak_freq', 'az_filtered_peak_freq',\n",
        "    'ax_filtered_peak_mag', 'ay_filtered_peak_mag', 'az_filtered_peak_mag',\n",
        "    'ax_filtered_mean_mag', 'ay_filtered_mean_mag', 'az_filtered_mean_mag'\n",
        "]\n",
        "\n",
        "def simple_bandpass_filter(data, fs, lowcut=25.0, highcut=65.0, order=4):\n",
        "    nyquist = 0.5 * fs\n",
        "    low = lowcut / nyquist\n",
        "    high = highcut / nyquist\n",
        "    if high >= 1.0:\n",
        "        high = 0.99\n",
        "    b, a = butter(order, [low, high], btype='band')\n",
        "    y = filtfilt(b, a, data)\n",
        "    return y\n",
        "\n",
        "def calculate_sampling_frequency(time_data):\n",
        "    if len(time_data) < 2:\n",
        "        return 100.0\n",
        "    avg_interval = np.mean(np.diff(time_data))\n",
        "    fs = round(1.0 / avg_interval, 2) if avg_interval > 0 else 100.0\n",
        "    return fs\n",
        "\n",
        "def load_and_analyze_sensor_file(file_name, dataset='Training'):\n",
        "    if dataset == 'Training':\n",
        "        file_path = os.path.join(training_data_path, file_name)\n",
        "    else:\n",
        "        file_path = os.path.join(testing_data_path, file_name)\n",
        "\n",
        "    if not os.path.exists(file_path):\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        df = pd.read_excel(file_path, engine='xlrd')\n",
        "        df.columns = ['Time', 'ax', 'ay', 'az', 'a_abs']\n",
        "\n",
        "        fs = calculate_sampling_frequency(df['Time'].values)\n",
        "\n",
        "        file_metadata = frequency_df[frequency_df['File Name'] == file_name]\n",
        "        if not file_metadata.empty:\n",
        "            metadata = file_metadata.iloc[0]\n",
        "            ground_truth_freq = metadata.get('Ground_Truth_Freq', 'Unknown')\n",
        "            has_defect = metadata.get('Has_Frequency_Defect', 0)\n",
        "            case_id = metadata.get('Case_ID', 'Unknown')\n",
        "            sensor_pos = metadata.get('Sensor Position', 'Unknown')\n",
        "        else:\n",
        "            ground_truth_freq = 'Unknown'\n",
        "            has_defect = 0\n",
        "            case_id = 'Unknown'\n",
        "            sensor_pos = 'Unknown'\n",
        "\n",
        "        return {\n",
        "            'data': df,\n",
        "            'fs': fs,\n",
        "            'metadata': {\n",
        "                'file_name': file_name,\n",
        "                'dataset': dataset,\n",
        "                'case_id': case_id,\n",
        "                'sensor_position': sensor_pos,\n",
        "                'ground_truth_freq': ground_truth_freq,\n",
        "                'has_defect': has_defect\n",
        "            }\n",
        "        }\n",
        "\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def create_comprehensive_fft_plot(file_info):\n",
        "    if file_info is None:\n",
        "        return\n",
        "\n",
        "    df = file_info['data']\n",
        "    fs = file_info['fs']\n",
        "    metadata = file_info['metadata']\n",
        "\n",
        "    fig = plt.figure(figsize=(20, 15))\n",
        "    colors = {'ax': 'red', 'ay': 'green', 'az': 'blue'}\n",
        "\n",
        "    plt.subplot(3, 3, 1)\n",
        "    for axis in ['ax', 'ay', 'az']:\n",
        "        plt.plot(df['Time'], df[axis], label=f'{axis.upper()}', color=colors[axis], alpha=0.7)\n",
        "    plt.title('Raw Acceleration Data')\n",
        "    plt.xlabel('Time (s)')\n",
        "    plt.ylabel('Acceleration')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.subplot(3, 3, 2)\n",
        "    for axis in ['ax', 'ay', 'az']:\n",
        "        filtered_data = simple_bandpass_filter(df[axis].values, fs)\n",
        "        plt.plot(df['Time'], filtered_data, label=f'{axis.upper()} Filtered', color=colors[axis], alpha=0.7)\n",
        "    plt.title('Bandpass Filtered Data (25-65 Hz)')\n",
        "    plt.xlabel('Time (s)')\n",
        "    plt.ylabel('Filtered Acceleration')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.subplot(3, 3, 3)\n",
        "    for axis in ['ax', 'ay', 'az']:\n",
        "        N = len(df[axis])\n",
        "        yf = fft(df[axis].values)\n",
        "        xf = fftfreq(N, 1 / fs)\n",
        "        pos_mask = xf >= 0\n",
        "        xf_pos = xf[pos_mask]\n",
        "        yf_pos = np.abs(yf[pos_mask])\n",
        "        plt.semilogy(xf_pos, yf_pos, label=f'{axis.upper()}', color=colors[axis], alpha=0.7)\n",
        "    plt.title('Full FFT Spectrum (0-50 Hz)')\n",
        "    plt.xlabel('Frequency (Hz)')\n",
        "    plt.ylabel('Magnitude (log scale)')\n",
        "    plt.xlim(0, 50)\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    for i, axis in enumerate(['ax', 'ay', 'az']):\n",
        "        plt.subplot(3, 3, 4 + i)\n",
        "        N = len(df[axis])\n",
        "        yf = fft(df[axis].values)\n",
        "        xf = fftfreq(N, 1 / fs)\n",
        "        pos_mask = xf >= 0\n",
        "        xf_pos = xf[pos_mask]\n",
        "        yf_pos = np.abs(yf[pos_mask])\n",
        "        freq_mask = (xf_pos >= 20) & (xf_pos <= 70)\n",
        "        xf_defect = xf_pos[freq_mask]\n",
        "        yf_defect = yf_pos[freq_mask]\n",
        "        plt.plot(xf_defect, yf_defect, color=colors[axis], linewidth=2)\n",
        "        if len(yf_defect) > 0:\n",
        "            peak_idx = np.argmax(yf_defect)\n",
        "            peak_freq = xf_defect[peak_idx]\n",
        "            peak_mag = yf_defect[peak_idx]\n",
        "            plt.axvline(x=peak_freq, color=colors[axis], linestyle='--', alpha=0.8)\n",
        "            plt.text(peak_freq + 1, peak_mag * 0.8, f'{peak_freq:.1f} Hz', color=colors[axis], fontweight='bold')\n",
        "        if metadata['ground_truth_freq'] != 'Unknown' and metadata['ground_truth_freq'] is not None:\n",
        "            gt_freq = float(metadata['ground_truth_freq'])\n",
        "            if 20 <= gt_freq <= 70:\n",
        "                plt.axvline(x=gt_freq, color='black', linestyle='-', alpha=0.5, linewidth=2)\n",
        "                plt.text(gt_freq + 1, max(yf_defect) * 0.9, f'GT: {gt_freq} Hz', color='black', fontweight='bold')\n",
        "        plt.title(f'{axis.upper()} Axis - Defect Range (20-70 Hz)')\n",
        "        plt.xlabel('Frequency (Hz)')\n",
        "        plt.ylabel('Magnitude')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.subplot(3, 3, 7)\n",
        "    for axis in ['ax', 'ay', 'az']:\n",
        "        filtered_data = simple_bandpass_filter(df[axis].values, fs)\n",
        "        N = len(filtered_data)\n",
        "        yf = fft(filtered_data)\n",
        "        xf = fftfreq(N, 1 / fs)\n",
        "        pos_mask = xf >= 0\n",
        "        xf_pos = xf[pos_mask]\n",
        "        yf_pos = np.abs(yf[pos_mask])\n",
        "        freq_mask = (xf_pos >= 20) & (xf_pos <= 70)\n",
        "        xf_filt = xf_pos[freq_mask]\n",
        "        yf_filt = yf_pos[freq_mask]\n",
        "        plt.plot(xf_filt, yf_filt, label=f'{axis.upper()} Filtered', color=colors[axis], linewidth=2)\n",
        "    plt.title('Filtered Signal FFT (25-65 Hz Bandpass)')\n",
        "    plt.xlabel('Frequency (Hz)')\n",
        "    plt.ylabel('Magnitude')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.subplot(3, 3, 8)\n",
        "    stats_data = []\n",
        "    for axis in ['ax', 'ay', 'az']:\n",
        "        filtered_data = simple_bandpass_filter(df[axis].values, fs)\n",
        "        N = len(filtered_data)\n",
        "        yf = fft(filtered_data)\n",
        "        xf = fftfreq(N, 1 / fs)\n",
        "        pos_mask = xf >= 0\n",
        "        freq_mask = (xf[pos_mask] >= 25) & (xf[pos_mask] <= 65)\n",
        "        yf_range = np.abs(yf[pos_mask])[freq_mask]\n",
        "        peak_idx = np.argmax(yf_range)\n",
        "        peak_freq = xf[pos_mask][freq_mask][peak_idx]\n",
        "        peak_mag = yf_range[peak_idx]\n",
        "        mean_mag = np.mean(yf_range)\n",
        "        stats_data.append({\n",
        "            'Axis': axis.upper(),\n",
        "            'Peak Freq (Hz)': f'{peak_freq:.1f}',\n",
        "            'Peak Magnitude': f'{peak_mag:.0f}',\n",
        "            'Mean Magnitude': f'{mean_mag:.0f}',\n",
        "            'Peak/Mean Ratio': f'{peak_mag/mean_mag:.2f}'\n",
        "        })\n",
        "    stats_df = pd.DataFrame(stats_data)\n",
        "    table = plt.table(cellText=stats_df.values, colLabels=stats_df.columns,\n",
        "                     cellLoc='center', loc='center', bbox=[0, 0, 1, 1])\n",
        "    table.auto_set_font_size(False)\n",
        "    table.set_fontsize(10)\n",
        "    plt.axis('off')\n",
        "    plt.title('Frequency Analysis Summary')\n",
        "\n",
        "    plt.subplot(3, 3, 9)\n",
        "    metrics = []\n",
        "    for axis in ['ax', 'ay', 'az']:\n",
        "        signal = df[axis].values\n",
        "        signal_power = np.mean(signal**2)\n",
        "        noise_power = np.var(np.diff(signal))\n",
        "        snr = 10 * np.log10(signal_power / max(noise_power, 1e-10))\n",
        "        rms = np.sqrt(np.mean(signal**2))\n",
        "        peak_to_peak = np.max(signal) - np.min(signal)\n",
        "        metrics.append([axis.upper(), f'{snr:.1f}', f'{rms:.3f}', f'{peak_to_peak:.3f}'])\n",
        "    quality_df = pd.DataFrame(metrics, columns=['Axis', 'SNR (dB)', 'RMS', 'Peak-to-Peak'])\n",
        "    table2 = plt.table(cellText=quality_df.values, colLabels=quality_df.columns,\n",
        "                      cellLoc='center', loc='center', bbox=[0, 0, 1, 1])\n",
        "    table2.auto_set_font_size(False)\n",
        "    table2.set_fontsize(10)\n",
        "    plt.axis('off')\n",
        "    plt.title('Signal Quality Metrics')\n",
        "\n",
        "    defect_status = \"WITH DEFECT\" if metadata['has_defect'] == 1 else \"NO DEFECT\"\n",
        "    fig.suptitle(f'FFT Analysis: {metadata[\"file_name\"]} | Case {metadata[\"case_id\"]} | '\n",
        "                f'Sensor {metadata[\"sensor_position\"]} | {defect_status} | '\n",
        "                f'GT: {metadata[\"ground_truth_freq\"]} Hz | FS: {fs} Hz',\n",
        "                fontsize=16, fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.subplots_adjust(top=0.93)\n",
        "    plt.show()\n",
        "\n",
        "def list_available_files():\n",
        "    # Training files\n",
        "    training_files = frequency_df[frequency_df['Dataset'] == 'Training']['File Name'].unique()\n",
        "    # Testing files\n",
        "    testing_files = frequency_df[frequency_df['Dataset'] == 'Testing']['File Name'].unique()\n",
        "    return training_files, testing_files\n",
        "\n",
        "def visualize_sensor_file(file_name, dataset='Training'):\n",
        "    file_info = load_and_analyze_sensor_file(file_name, dataset)\n",
        "    if file_info:\n",
        "        create_comprehensive_fft_plot(file_info)\n",
        "\n",
        "training_files, testing_files = list_available_files()\n",
        "visualize_sensor_file('G6_P5_case64.xls', 'Training')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RANDOM FOREST FOR FREQUENCY DEFECT CLASSIFICATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJOylQwd1OnN"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "classification_data = training_data.dropna(subset=['Has_Frequency_Defect'] + feature_cols)\n",
        "classification_data['Has_Frequency_Defect'] = classification_data['Has_Frequency_Defect'].astype(int)\n",
        "\n",
        "X_clf = classification_data[feature_cols].values\n",
        "y_clf = classification_data['Has_Frequency_Defect'].values\n",
        "\n",
        "print(f\"Dataset Info:\")\n",
        "print(f\"Total samples: {len(classification_data)}\")\n",
        "print(f\"Class distribution: {np.bincount(y_clf)} (No Defect: {np.bincount(y_clf)[0]}, Has Defect: {np.bincount(y_clf)[1]})\")\n",
        "\n",
        "X_train_clf, X_val_clf, y_train_clf, y_val_clf = train_test_split(\n",
        "    X_clf, y_clf, test_size=0.2, random_state=42, stratify=y_clf\n",
        ")\n",
        "\n",
        "scaler_clf = StandardScaler()\n",
        "X_train_clf_scaled = scaler_clf.fit_transform(X_train_clf)\n",
        "X_val_clf_scaled = scaler_clf.transform(X_val_clf)\n",
        "\n",
        "clf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf_model.fit(X_train_clf_scaled, y_train_clf)\n",
        "\n",
        "y_val_clf_pred = clf_model.predict(X_val_clf_scaled)\n",
        "y_val_clf_proba = clf_model.predict_proba(X_val_clf_scaled)[:, 1]\n",
        "\n",
        "print(f\"\\nCLASSIFICATION METRICS:\")\n",
        "clf_accuracy = accuracy_score(y_val_clf, y_val_clf_pred)\n",
        "precision = precision_score(y_val_clf, y_val_clf_pred)\n",
        "recall = recall_score(y_val_clf, y_val_clf_pred)\n",
        "f1 = f1_score(y_val_clf, y_val_clf_pred)\n",
        "auc_score = roc_auc_score(y_val_clf, y_val_clf_proba)\n",
        "\n",
        "print(f\"Accuracy:  {clf_accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "print(f\"F1-Score:  {f1:.4f}\")\n",
        "print(f\"AUC-ROC:   {auc_score:.4f}\")\n",
        "\n",
        "cm = confusion_matrix(y_val_clf, y_val_clf_pred)\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "print(f\"\\nCONFUSION MATRIX:\")\n",
        "print(f\"                 Predicted\")\n",
        "print(f\"               No Defect  Has Defect\")\n",
        "print(f\"Actual No      {tn:8d}  {fp:9d}\")\n",
        "print(f\"       Has     {fn:8d}  {tp:9d}\")\n",
        "\n",
        "sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "\n",
        "print(f\"\\nADVANCED METRICS:\")\n",
        "print(f\"Sensitivity (True Positive Rate): {sensitivity:.4f}\")\n",
        "print(f\"Specificity (True Negative Rate): {specificity:.4f}\")\n",
        "print(f\"False Positive Rate: {fp/(fp+tn):.4f}\")\n",
        "print(f\"False Negative Rate: {fn/(fn+tp):.4f}\")\n",
        "\n",
        "feature_importance = clf_model.feature_importances_\n",
        "feature_df = pd.DataFrame({\n",
        "    'Feature': feature_cols,\n",
        "    'Importance': feature_importance\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "print(f\"\\nFEATURE IMPORTANCE:\")\n",
        "for i, row in feature_df.iterrows():\n",
        "    print(f\"{row['Feature']:25} : {row['Importance']:.4f}\")\n",
        "\n",
        "train_accuracy = clf_model.score(X_train_clf_scaled, y_train_clf)\n",
        "print(f\"\\nOVERFITTING CHECK:\")\n",
        "print(f\"Training Accuracy:   {train_accuracy:.4f}\")\n",
        "print(f\"Validation Accuracy: {clf_accuracy:.4f}\")\n",
        "print(f\"Difference:          {abs(train_accuracy - clf_accuracy):.4f}\")\n",
        "\n",
        "high_conf = (y_val_clf_proba > 0.8) | (y_val_clf_proba < 0.2)\n",
        "medium_conf = ((y_val_clf_proba >= 0.2) & (y_val_clf_proba <= 0.4)) | ((y_val_clf_proba >= 0.6) & (y_val_clf_proba <= 0.8))\n",
        "low_conf = (y_val_clf_proba > 0.4) & (y_val_clf_proba < 0.6)\n",
        "\n",
        "print(f\"\\nPREDICTION CONFIDENCE:\")\n",
        "print(f\"High Confidence (>0.8 or <0.2):     {high_conf.sum():3d} samples ({high_conf.sum()/len(y_val_clf)*100:.1f}%)\")\n",
        "print(f\"Medium Confidence (0.2-0.4, 0.6-0.8): {medium_conf.sum():3d} samples ({medium_conf.sum()/len(y_val_clf)*100:.1f}%)\")\n",
        "print(f\"Low Confidence (0.4-0.6):           {low_conf.sum():3d} samples ({low_conf.sum()/len(y_val_clf)*100:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FMel7251yKk"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "classification_data = training_data.dropna(subset=['Has_Frequency_Defect'] + feature_cols)\n",
        "classification_data['Has_Frequency_Defect'] = classification_data['Has_Frequency_Defect'].astype(int)\n",
        "\n",
        "X_clf = classification_data[feature_cols].values\n",
        "y_clf = classification_data['Has_Frequency_Defect'].values\n",
        "\n",
        "print(f\"Dataset Info:\")\n",
        "print(f\"Total samples: {len(classification_data)}\")\n",
        "print(f\"Class distribution: {np.bincount(y_clf)} (No Defect: {np.bincount(y_clf)[0]}, Has Defect: {np.bincount(y_clf)[1]})\")\n",
        "\n",
        "X_train_clf, X_val_clf, y_train_clf, y_val_clf = train_test_split(\n",
        "    X_clf, y_clf, test_size=0.2, random_state=42, stratify=y_clf\n",
        ")\n",
        "\n",
        "scaler_clf = StandardScaler()\n",
        "X_train_clf_scaled = scaler_clf.fit_transform(X_train_clf)\n",
        "X_val_clf_scaled = scaler_clf.transform(X_val_clf)\n",
        "\n",
        "clf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf_model.fit(X_train_clf_scaled, y_train_clf)\n",
        "\n",
        "y_val_clf_pred = clf_model.predict(X_val_clf_scaled)\n",
        "y_val_clf_proba = clf_model.predict_proba(X_val_clf_scaled)[:, 1]\n",
        "\n",
        "print(f\"\\nCLASSIFICATION METRICS:\")\n",
        "clf_accuracy = accuracy_score(y_val_clf, y_val_clf_pred)\n",
        "precision = precision_score(y_val_clf, y_val_clf_pred)\n",
        "recall = recall_score(y_val_clf, y_val_clf_pred)\n",
        "f1 = f1_score(y_val_clf, y_val_clf_pred)\n",
        "auc_score = roc_auc_score(y_val_clf, y_val_clf_proba)\n",
        "\n",
        "print(f\"Accuracy:  {clf_accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "print(f\"F1-Score:  {f1:.4f}\")\n",
        "print(f\"AUC-ROC:   {auc_score:.4f}\")\n",
        "\n",
        "cm = confusion_matrix(y_val_clf, y_val_clf_pred)\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "print(f\"\\nCONFUSION MATRIX:\")\n",
        "print(f\"                 Predicted\")\n",
        "print(f\"               No Defect  Has Defect\")\n",
        "print(f\"Actual No      {tn:8d}  {fp:9d}\")\n",
        "print(f\"       Has     {fn:8d}  {tp:9d}\")\n",
        "\n",
        "sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "\n",
        "print(f\"\\nADVANCED METRICS:\")\n",
        "print(f\"Sensitivity (True Positive Rate): {sensitivity:.4f}\")\n",
        "print(f\"Specificity (True Negative Rate): {specificity:.4f}\")\n",
        "print(f\"False Positive Rate: {fp/(fp+tn):.4f}\")\n",
        "print(f\"False Negative Rate: {fn/(fn+tp):.4f}\")\n",
        "\n",
        "feature_importance = clf_model.feature_importances_\n",
        "feature_df = pd.DataFrame({\n",
        "    'Feature': feature_cols,\n",
        "    'Importance': feature_importance\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "print(f\"\\nFEATURE IMPORTANCE:\")\n",
        "for _, row in feature_df.iterrows():\n",
        "    print(f\"{row['Feature']:25} : {row['Importance']:.4f}\")\n",
        "\n",
        "train_accuracy = clf_model.score(X_train_clf_scaled, y_train_clf)\n",
        "print(f\"\\nOVERFITTING CHECK:\")\n",
        "print(f\"Training Accuracy:   {train_accuracy:.4f}\")\n",
        "print(f\"Validation Accuracy: {clf_accuracy:.4f}\")\n",
        "print(f\"Difference:          {abs(train_accuracy - clf_accuracy):.4f}\")\n",
        "\n",
        "high_conf = (y_val_clf_proba > 0.8) | (y_val_clf_proba < 0.2)\n",
        "medium_conf = ((y_val_clf_proba >= 0.2) & (y_val_clf_proba <= 0.4)) | ((y_val_clf_proba >= 0.6) & (y_val_clf_proba <= 0.8))\n",
        "low_conf = (y_val_clf_proba > 0.4) & (y_val_clf_proba < 0.6)\n",
        "\n",
        "print(f\"\\nPREDICTION CONFIDENCE:\")\n",
        "print(f\"High Confidence (>0.8 or <0.2):     {high_conf.sum():3d} samples ({high_conf.sum()/len(y_val_clf)*100:.1f}%)\")\n",
        "print(f\"Medium Confidence (0.2-0.4, 0.6-0.8): {medium_conf.sum():3d} samples ({medium_conf.sum()/len(y_val_clf)*100:.1f}%)\")\n",
        "print(f\"Low Confidence (0.4-0.6):           {low_conf.sum():3d} samples ({low_conf.sum()/len(y_val_clf)*100:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FREQUECY DEFECY VALUE REGRESSION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPevEhOT2FTO"
      },
      "outputs": [],
      "source": [
        "if len(testing_data) > 0:\n",
        "    testing_data_clean = testing_data.dropna(subset=feature_cols)\n",
        "    print(f\"Testing samples with complete features: {len(testing_data_clean)}\")\n",
        "\n",
        "    X_test = testing_data_clean[feature_cols].values\n",
        "    X_test_scaled = scaler_clf.transform(X_test)\n",
        "\n",
        "    predictions = clf_model.predict(X_test_scaled)\n",
        "    prediction_proba = clf_model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "    results_df = testing_data_clean.copy()\n",
        "    results_df['Predicted_Frequency_Defect'] = predictions\n",
        "    results_df['Prediction_Probability'] = prediction_proba\n",
        "\n",
        "    if has_regression_model:\n",
        "        defect_mask = predictions == 1\n",
        "        if defect_mask.sum() > 0:\n",
        "            X_defect_test = X_test[defect_mask]\n",
        "            X_defect_test_scaled = scaler_reg.transform(X_defect_test)\n",
        "            freq_predictions = reg_model.predict(X_defect_test_scaled)\n",
        "\n",
        "            results_df['Predicted_Frequency_Value'] = np.nan\n",
        "            results_df.loc[defect_mask, 'Predicted_Frequency_Value'] = freq_predictions\n",
        "        else:\n",
        "            results_df['Predicted_Frequency_Value'] = np.nan\n",
        "    else:\n",
        "        results_df['Predicted_Frequency_Value'] = np.nan\n",
        "\n",
        "    print(f\"Total testing samples processed: {len(results_df)}\")\n",
        "    print(f\"Predicted frequency defects: {predictions.sum()}\")\n",
        "    print(f\"Predicted no defects: {(predictions == 0).sum()}\")\n",
        "\n",
        "    summary_by_case = results_df.groupby(['Case_ID', 'Sensor Position']).agg({\n",
        "        'Predicted_Frequency_Defect': 'first',\n",
        "        'Prediction_Probability': 'first',\n",
        "        'Predicted_Frequency_Value': 'first'\n",
        "    }).round(2)\n",
        "\n",
        "    for case_id in sorted(results_df['Case_ID'].unique()):\n",
        "        case_data = summary_by_case.loc[case_id]\n",
        "        print(f\"\\nCase {case_id}:\")\n",
        "        for sensor, data in case_data.iterrows():\n",
        "            defect = \"YES\" if data['Predicted_Frequency_Defect'] == 1 else \"NO\"\n",
        "            prob = data['Prediction_Probability']\n",
        "            freq = data['Predicted_Frequency_Value']\n",
        "            freq_str = f\"{freq:.1f} Hz\" if not pd.isna(freq) else \"N/A\"\n",
        "            print(f\"  Sensor {sensor}: Defect={defect} (Prob={prob:.3f}) Freq={freq_str}\")\n",
        "\n",
        "    results_output_path = os.path.join(output_base_path, 'Optimized_Testing_Predictions.csv')\n",
        "    results_df.to_csv(results_output_path, index=False)\n",
        "    print(f\"\\n predictions saved to: {results_output_path}\")\n",
        "\n",
        "else:\n",
        "    print(\"No testing frequency data found!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FEATURE VISUALISATION FOR SECTION MAPPING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMiwix0C3uQG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import butter, filtfilt\n",
        "from scipy.fft import fft, fftfreq\n",
        "from scipy import stats\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Data and File Paths\n",
        "training_data_path = '/content/drive/MyDrive/IMES/imes/data_files/Raw Data/Training Data'\n",
        "output_base_path = '/content/drive/MyDrive/IMES/imes/processed'\n",
        "perfect_case_file = 'G0_P5_case_perfect.xls'\n",
        "file_path = os.path.join(training_data_path, perfect_case_file)\n",
        "\n",
        "# Load perfect case data\n",
        "df = pd.read_excel(file_path, engine='xlrd')\n",
        "df.columns = ['Time', 'ax', 'ay', 'az', 'a_abs']\n",
        "\n",
        "sampling_intervals = np.diff(df['Time'].values)\n",
        "fs = 1.0 / np.mean(sampling_intervals)\n",
        "\n",
        "# Apply bandpass filter\n",
        "def apply_bandpass_filter(data, fs, lowcut=25.0, highcut=65.0, order=4):\n",
        "    nyquist = 0.5 * fs\n",
        "    max_freq = nyquist * 0.99\n",
        "    lowcut = min(lowcut, max_freq * 0.5)\n",
        "    highcut = min(highcut, max_freq)\n",
        "    if lowcut <= 0:\n",
        "        lowcut = 1.0\n",
        "    if highcut <= lowcut:\n",
        "        highcut = lowcut + 10.0\n",
        "    low = lowcut / nyquist\n",
        "    high = highcut / nyquist\n",
        "    low = max(0.01, min(low, 0.98))\n",
        "    high = max(low + 0.01, min(high, 0.99))\n",
        "    b, a = butter(order, [low, high], btype='band')\n",
        "    return filtfilt(b, a, data)\n",
        "\n",
        "ax_filtered = apply_bandpass_filter(df['ax'].values, fs)\n",
        "ay_filtered = apply_bandpass_filter(df['ay'].values, fs)\n",
        "az_filtered = apply_bandpass_filter(df['az'].values, fs)\n",
        "ax_norm = (ax_filtered - np.mean(ax_filtered)) / np.std(ax_filtered)\n",
        "ay_norm = (ay_filtered - np.mean(ay_filtered)) / np.std(ay_filtered)\n",
        "az_norm = (az_filtered - np.mean(az_filtered)) / np.std(az_filtered)\n",
        "abs_acceleration = np.sqrt(ax_norm**2 + ay_norm**2 + az_norm**2)\n",
        "time_values = df['Time'].values\n",
        "\n",
        "def calculate_features(signal, time, window_size=100):\n",
        "    features = {\n",
        "        'time': [],\n",
        "        'rms': [],\n",
        "        'variance': [],\n",
        "        'std': [],\n",
        "        'mean_abs_dev': [],\n",
        "        'peak_to_peak': [],\n",
        "        'zero_crossings': [],\n",
        "        'skewness': [],\n",
        "        'kurtosis': [],\n",
        "        'percentile_25': [],\n",
        "        'percentile_75': [],\n",
        "        'iqr': [],\n",
        "        'velocity': [],\n",
        "        'acceleration_change': [],\n",
        "        'jerk': [],\n",
        "        'signal_energy': [],\n",
        "        'log_energy': [],\n",
        "        'dominant_freq': [],\n",
        "        'spectral_centroid': [],\n",
        "        'spectral_energy': []\n",
        "    }\n",
        "\n",
        "    velocity = np.gradient(signal, time)\n",
        "    acceleration_change = np.gradient(velocity, time)\n",
        "    jerk = np.gradient(acceleration_change, time)\n",
        "\n",
        "    for i in range(len(signal)):\n",
        "        start_idx = max(0, i - window_size//2)\n",
        "        end_idx = min(len(signal), i + window_size//2)\n",
        "        window_signal = signal[start_idx:end_idx]\n",
        "        window_time = time[start_idx:end_idx]\n",
        "        if len(window_signal) < 10:\n",
        "            for key in features.keys():\n",
        "                if key != 'time':\n",
        "                    features[key].append(np.nan)\n",
        "            features['time'].append(time[i])\n",
        "            continue\n",
        "        features['time'].append(time[i])\n",
        "        features['rms'].append(np.sqrt(np.mean(window_signal**2)))\n",
        "        features['variance'].append(np.var(window_signal))\n",
        "        features['std'].append(np.std(window_signal))\n",
        "        features['mean_abs_dev'].append(np.mean(np.abs(window_signal - np.mean(window_signal))))\n",
        "        features['peak_to_peak'].append(np.max(window_signal) - np.min(window_signal))\n",
        "        zero_cross = np.sum(np.diff(np.signbit(window_signal - np.mean(window_signal))))\n",
        "        features['zero_crossings'].append(zero_cross)\n",
        "        features['skewness'].append(stats.skew(window_signal))\n",
        "        features['kurtosis'].append(stats.kurtosis(window_signal))\n",
        "        features['percentile_25'].append(np.percentile(window_signal, 25))\n",
        "        features['percentile_75'].append(np.percentile(window_signal, 75))\n",
        "        features['iqr'].append(np.percentile(window_signal, 75) - np.percentile(window_signal, 25))\n",
        "        features['velocity'].append(np.abs(velocity[i]))\n",
        "        features['acceleration_change'].append(np.abs(acceleration_change[i]))\n",
        "        features['jerk'].append(np.abs(jerk[i]))\n",
        "        signal_energy = np.sum(window_signal**2)\n",
        "        features['signal_energy'].append(signal_energy)\n",
        "        features['log_energy'].append(np.log(signal_energy + 1e-8))\n",
        "        if len(window_signal) > 20:\n",
        "            fft_vals = np.abs(fft(window_signal))[:len(window_signal)//2]\n",
        "            freqs = fftfreq(len(window_signal), 1/fs)[:len(window_signal)//2]\n",
        "            if len(fft_vals) > 0 and np.sum(fft_vals) > 0:\n",
        "                dominant_idx = np.argmax(fft_vals)\n",
        "                features['dominant_freq'].append(freqs[dominant_idx] if dominant_idx < len(freqs) else 0)\n",
        "                spectral_centroid = np.sum(freqs * fft_vals) / np.sum(fft_vals)\n",
        "                features['spectral_centroid'].append(spectral_centroid)\n",
        "                features['spectral_energy'].append(np.sum(fft_vals**2))\n",
        "            else:\n",
        "                features['dominant_freq'].append(0)\n",
        "                features['spectral_centroid'].append(0)\n",
        "                features['spectral_energy'].append(0)\n",
        "        else:\n",
        "            features['dominant_freq'].append(0)\n",
        "            features['spectral_centroid'].append(0)\n",
        "            features['spectral_energy'].append(0)\n",
        "    return features\n",
        "\n",
        "features = calculate_features(abs_acceleration, time_values, window_size=200)\n",
        "features_df = pd.DataFrame(features)\n",
        "known_stationary = [\n",
        "    {'name': 'Location 8', 'start': 2, 'end': 8, 'center': 5},\n",
        "    {'name': 'Location 5', 'start': 25, 'end': 28, 'center': 26.5},\n",
        "    {'name': 'Location 2', 'start': 40, 'end': 43, 'center': 41.5}\n",
        "]\n",
        "\n",
        "fig = plt.figure(figsize=(20, 28))\n",
        "feature_groups = {\n",
        "    'Time Domain': ['rms', 'variance', 'std', 'mean_abs_dev'],\n",
        "    'Statistical': ['skewness', 'kurtosis', 'percentile_25', 'percentile_75'],\n",
        "    'Motion': ['velocity', 'acceleration_change', 'jerk'],\n",
        "    'Energy': ['signal_energy', 'log_energy'],\n",
        "    'Frequency': ['dominant_freq', 'spectral_centroid', 'spectral_energy'],\n",
        "    'Others': ['peak_to_peak', 'zero_crossings', 'iqr']\n",
        "}\n",
        "plot_idx = 1\n",
        "colors = ['blue', 'green', 'red', 'purple', 'orange', 'brown']\n",
        "\n",
        "for group_idx, (group_name, feature_list) in enumerate(feature_groups.items()):\n",
        "    for feature_idx, feature in enumerate(feature_list):\n",
        "        ax = plt.subplot(6, 4, plot_idx)\n",
        "        feature_data = features_df[feature].values\n",
        "        feature_data = feature_data[~np.isnan(feature_data)]\n",
        "        time_data = features_df['time'].values[:len(feature_data)]\n",
        "        if len(feature_data) > 0:\n",
        "            ax.plot(time_data, feature_data, color=colors[group_idx], linewidth=1.5, alpha=0.8)\n",
        "            for i, period in enumerate(known_stationary):\n",
        "                ax.axvspan(period['start'], period['end'], alpha=0.2, color=f'C{i}')\n",
        "                ax.text(period['center'], np.max(feature_data)*0.9,\n",
        "                        period['name'], ha='center', fontsize=8, weight='bold')\n",
        "            stationary_scores = []\n",
        "            for period in known_stationary:\n",
        "                mask = (time_data >= period['start']) & (time_data <= period['end'])\n",
        "                if np.any(mask):\n",
        "                    stationary_data = feature_data[mask]\n",
        "                    non_stationary_data = feature_data[~mask]\n",
        "                    if len(stationary_data) > 0 and len(non_stationary_data) > 0:\n",
        "                        stat_mean = np.mean(stationary_data)\n",
        "                        non_stat_mean = np.mean(non_stationary_data)\n",
        "                        separation = abs(stat_mean - non_stat_mean) / (np.std(feature_data) + 1e-8)\n",
        "                        stationary_scores.append(separation)\n",
        "            avg_score = np.mean(stationary_scores) if stationary_scores else 0\n",
        "            if avg_score > 1.0:\n",
        "                ax.set_title(f'{feature.replace(\"_\", \" \").title()}\\n(Sep Score: {avg_score:.2f})', fontsize=10, weight='bold', color='green')\n",
        "            elif avg_score > 0.5:\n",
        "                ax.set_title(f'{feature.replace(\"_\", \" \").title()}\\n(Sep Score: {avg_score:.2f})', fontsize=10, weight='bold', color='orange')\n",
        "            else:\n",
        "                ax.set_title(f'{feature.replace(\"_\", \" \").title()}\\n(Sep Score: {avg_score:.2f})', fontsize=10, weight='bold', color='red')\n",
        "            ax.set_ylabel(f'{feature}', fontsize=8)\n",
        "            ax.grid(True, alpha=0.3)\n",
        "        plot_idx += 1\n",
        "        if plot_idx > 24:\n",
        "            break\n",
        "    if plot_idx > 24:\n",
        "        break\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('Feature Types for Stationary Point Detection - Perfect Case\\n(Green=Excellent, Orange=Good, Red=Poor separation)',\n",
        "             fontsize=16, weight='bold', y=0.995)\n",
        "plt.subplots_adjust(top=0.96)\n",
        "\n",
        "viz_path = os.path.join(output_base_path, 'stationary_features_analysis.png')\n",
        "plt.savefig(viz_path, dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "feature_effectiveness = {}\n",
        "for feature in features_df.columns:\n",
        "    if feature == 'time':\n",
        "        continue\n",
        "    feature_data = features_df[feature].values\n",
        "    feature_data = feature_data[~np.isnan(feature_data)]\n",
        "    time_data = features_df['time'].values[:len(feature_data)]\n",
        "    if len(feature_data) == 0:\n",
        "        continue\n",
        "    stationary_scores = []\n",
        "    for period in known_stationary:\n",
        "        mask = (time_data >= period['start']) & (time_data <= period['end'])\n",
        "        if np.any(mask):\n",
        "            stationary_data = feature_data[mask]\n",
        "            non_stationary_data = feature_data[~mask]\n",
        "            if len(stationary_data) > 0 and len(non_stationary_data) > 0:\n",
        "                stat_mean = np.mean(stationary_data)\n",
        "                non_stat_mean = np.mean(non_stationary_data)\n",
        "                separation = abs(stat_mean - non_stat_mean) / (np.std(feature_data) + 1e-8)\n",
        "                stationary_scores.append(separation)\n",
        "    avg_score = np.mean(stationary_scores) if stationary_scores else 0\n",
        "    feature_effectiveness[feature] = avg_score\n",
        "\n",
        "sorted_features = sorted(feature_effectiveness.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(\"Rank | Feature Name           | Separation Score | Category\")\n",
        "print(\"-\" * 60)\n",
        "for i, (feature, score) in enumerate(sorted_features[:15], 1):\n",
        "    category = \"Excellent\" if score > 1.0 else \"Good\" if score > 0.5 else \"Poor\"\n",
        "    print(f\"{i:2d}   | {feature:<20} | {score:8.3f}     | {category}\")\n",
        "\n",
        "print(\"\\nTOP 5 FEATURES FOR STATIONARY DETECTION:\")\n",
        "for i, (feature, score) in enumerate(sorted_features[:5], 1):\n",
        "    print(f\"   {i}. {feature.replace('_', ' ').title()}: {score:.3f}\")\n",
        "\n",
        "print(\"\\nFEATURE GROUP EFFECTIVENESS:\")\n",
        "for group_name, feature_list in feature_groups.items():\n",
        "    group_scores = [feature_effectiveness.get(f, 0) for f in feature_list]\n",
        "    avg_group_score = np.mean([s for s in group_scores if s > 0])\n",
        "    print(f\"   {group_name}: {avg_group_score:.3f}\")\n",
        "\n",
        "print(\"\\nPERIOD-SPECIFIC FEATURE ANALYSIS:\")\n",
        "period_feature_matrix = []\n",
        "feature_names = []\n",
        "for feature in sorted_features[:10]:\n",
        "    feature_name = feature[0]\n",
        "    feature_names.append(feature_name)\n",
        "    feature_data = features_df[feature_name].values\n",
        "    feature_data = feature_data[~np.isnan(feature_data)]\n",
        "    time_data = features_df['time'].values[:len(feature_data)]\n",
        "    period_scores = []\n",
        "    for period in known_stationary:\n",
        "        mask = (time_data >= period['start']) & (time_data <= period['end'])\n",
        "        if np.any(mask):\n",
        "            stationary_data = feature_data[mask]\n",
        "            non_stationary_data = feature_data[~mask]\n",
        "            if len(stationary_data) > 0 and len(non_stationary_data) > 0:\n",
        "                stat_mean = np.mean(stationary_data)\n",
        "                non_stat_mean = np.mean(non_stationary_data)\n",
        "                separation = abs(stat_mean - non_stat_mean) / (np.std(feature_data) + 1e-8)\n",
        "                period_scores.append(separation)\n",
        "            else:\n",
        "                period_scores.append(0)\n",
        "        else:\n",
        "            period_scores.append(0)\n",
        "    period_feature_matrix.append(period_scores)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "period_names = [p['name'] for p in known_stationary]\n",
        "sns.heatmap(period_feature_matrix,\n",
        "            xticklabels=period_names,\n",
        "            yticklabels=[f.replace('_', ' ').title() for f in feature_names],\n",
        "            annot=True, fmt='.2f', cmap='viridis')\n",
        "plt.title('Feature Effectiveness by Stationary Period', fontsize=14, weight='bold')\n",
        "plt.xlabel('Stationary Periods')\n",
        "plt.ylabel('Features')\n",
        "plt.tight_layout()\n",
        "\n",
        "heatmap_path = os.path.join(output_base_path, 'feature_effectiveness_heatmap.png')\n",
        "plt.savefig(heatmap_path, dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"Visualizations saved:\")\n",
        "print(f\"   Main analysis: {viz_path}\")\n",
        "print(f\"   Effectiveness heatmap: {heatmap_path}\")\n",
        "\n",
        "print(f\"ANALYSIS COMPLETE\")\n",
        "print(f\"   • {len(feature_effectiveness)} features analyzed\")\n",
        "print(f\"   • {len(known_stationary)} stationary periods identified\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# VISUALISING SECTION SEGREGATION BASD ON SELECTED A-25 FEATURE AND SELECTING THRESHOLDS AND TIME SECTIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faD8dA2-30D9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import butter, filtfilt\n",
        "from scipy.fft import fft, fftfreq\n",
        "from scipy import stats\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def load_data(case_file, dataset_type='training'):\n",
        "    \"\"\"Load data from specified case file.\"\"\"\n",
        "    if dataset_type.lower() == 'training':\n",
        "        data_path = '/content/drive/MyDrive/IMES/imes/data_files/Raw Data/Training Data'\n",
        "    else:\n",
        "        data_path = '/content/drive/MyDrive/IMES/imes/data_files/Raw Data/Testing Data'\n",
        "    file_path = os.path.join(data_path, case_file)\n",
        "    df = pd.read_excel(file_path, engine='xlrd')\n",
        "    df.columns = ['Time', 'ax', 'ay', 'az', 'a_abs']\n",
        "    return df\n",
        "\n",
        "def calculate_features(df):\n",
        "    \"\"\"Calculate P25 features from acceleration data.\"\"\"\n",
        "    time_values = df['Time'].values\n",
        "    abs_acceleration = np.sqrt(df['ax']**2 + df['ay']**2 + df['az']**2)\n",
        "    p25_values = []\n",
        "    window_size = 200\n",
        "    for i in range(len(abs_acceleration)):\n",
        "        start_idx = max(0, i - window_size//2)\n",
        "        end_idx = min(len(abs_acceleration), i + window_size//2)\n",
        "        window_signal = abs_acceleration[start_idx:end_idx]\n",
        "        if len(window_signal) >= 10:\n",
        "            p25 = np.percentile(window_signal, 25)\n",
        "            p25_values.append(p25)\n",
        "        else:\n",
        "            p25_values.append(p25_values[-1] if p25_values else 0)\n",
        "    return time_values, abs_acceleration, np.array(p25_values)\n",
        "\n",
        "def detect_stationary_with_center_cropping(p25_signal, time_vals, start_time, end_time,\n",
        "                                          threshold_factor, min_stationary_duration,\n",
        "                                          max_allowed_duration):\n",
        "    \"\"\"\n",
        "    Detect stationary period; crop from center if period is too long.\n",
        "    \"\"\"\n",
        "    mask = (time_vals >= start_time) & (time_vals <= end_time)\n",
        "    window_p25 = p25_signal[mask]\n",
        "    window_time = time_vals[mask]\n",
        "    if len(window_p25) < 20:\n",
        "        return None\n",
        "\n",
        "    baseline = np.percentile(window_p25, 25)\n",
        "    threshold = baseline * threshold_factor\n",
        "    stationary_mask = window_p25 <= threshold\n",
        "    best_start, best_end, best_duration = None, None, 0\n",
        "    current_start = None\n",
        "\n",
        "    for i, is_stationary in enumerate(stationary_mask):\n",
        "        if is_stationary and current_start is None:\n",
        "            current_start = i\n",
        "        elif not is_stationary and current_start is not None:\n",
        "            duration = window_time[i-1] - window_time[current_start]\n",
        "            if duration > best_duration and duration >= min_stationary_duration:\n",
        "                best_duration = duration\n",
        "                best_start = window_time[current_start]\n",
        "                best_end = window_time[i-1]\n",
        "            current_start = None\n",
        "\n",
        "    if current_start is not None:\n",
        "        duration = window_time[-1] - window_time[current_start]\n",
        "        if duration > best_duration and duration >= min_stationary_duration:\n",
        "            best_duration = duration\n",
        "            best_start = window_time[current_start]\n",
        "            best_end = window_time[-1]\n",
        "\n",
        "    if best_start is not None:\n",
        "        original_center = (best_start + best_end) / 2\n",
        "        original_duration = best_duration\n",
        "        if best_duration > max_allowed_duration:\n",
        "            half_allowed = max_allowed_duration / 2\n",
        "            new_start = max(original_center - half_allowed, best_start)\n",
        "            new_end = min(original_center + half_allowed, best_end)\n",
        "            best_start = new_start\n",
        "            best_end = new_end\n",
        "            best_duration = new_end - new_start\n",
        "        center_time = (best_start + best_end) / 2\n",
        "        final_mask = (window_time >= best_start) & (window_time <= best_end)\n",
        "        avg_p25 = np.mean(window_p25[final_mask]) if np.any(final_mask) else baseline\n",
        "        return {\n",
        "            'start_time': best_start, 'end_time': best_end, 'center_time': center_time,\n",
        "            'duration': best_duration, 'avg_p25': avg_p25, 'threshold': threshold,\n",
        "            'min_duration_met': best_duration >= min_stationary_duration,\n",
        "            'was_cropped': original_duration > max_allowed_duration,\n",
        "            'original_duration': original_duration,\n",
        "            'max_duration_allowed': max_allowed_duration\n",
        "        }\n",
        "    return None\n",
        "\n",
        "def detect_transition_peak_between_5_and_2(p25_signal, time_vals, search_start, search_end):\n",
        "    \"\"\"Detect transition peak specifically between Location 5 and Location 2.\"\"\"\n",
        "    mask = (time_vals >= search_start) & (time_vals <= search_end)\n",
        "    if not np.any(mask):\n",
        "        return None\n",
        "    transition_p25 = p25_signal[mask]\n",
        "    transition_time = time_vals[mask]\n",
        "    if len(transition_p25) < 5:\n",
        "        return None\n",
        "    max_idx = np.argmax(transition_p25)\n",
        "    return {\n",
        "        'name': 'Peak Transition (5→2)',\n",
        "        'time': transition_time[max_idx],\n",
        "        'value': transition_p25[max_idx]\n",
        "    }\n",
        "\n",
        "def create_clean_visualization(time_values, abs_acceleration, p25_signal,\n",
        "                              detected_points, transition_point, case_file, dataset_type):\n",
        "    \"\"\"Visualization with cropping results.\"\"\"\n",
        "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(18, 14))\n",
        "    dataset_label = \"TRAINING\" if dataset_type.lower() == 'training' else \"TESTING\"\n",
        "    ax1.plot(time_values, abs_acceleration, 'b-', linewidth=1.5, label='Raw Acceleration')\n",
        "    ax1.set_title(f'{dataset_label} {case_file} - Raw Acceleration', fontsize=12, weight='bold')\n",
        "    ax1.set_ylabel('Raw Acceleration')\n",
        "    ax1.set_xlabel('Time (s)')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    ax2.plot(time_values, p25_signal, 'purple', linewidth=2, label='P25 Signal')\n",
        "    colors = ['blue', 'orange', 'green']\n",
        "    for i, point in enumerate(detected_points):\n",
        "        color = colors[i % len(colors)]\n",
        "        ax2.axvspan(point['start_time'], point['end_time'], alpha=0.4, color=color)\n",
        "        ax2.axhline(y=point['threshold'], color=color, linestyle='--', linewidth=2, alpha=0.8)\n",
        "        crop_indicator = \"(Cropped)\" if point.get('was_cropped', False) else \"(Original)\"\n",
        "        original_dur = point.get('original_duration', point['duration'])\n",
        "        info_text = (f\"{point['name']} {crop_indicator}\\n\"\n",
        "                    f\"Time: {point['center_time']:.1f}s\\n\"\n",
        "                    f\"Final: {point['duration']:.1f}s\\n\"\n",
        "                    f\"Original: {original_dur:.1f}s\\n\"\n",
        "                    f\"Max: {point['max_duration_allowed']:.1f}s\")\n",
        "        ax2.text(point['center_time'], point['avg_p25'],\n",
        "                info_text,\n",
        "                ha='center', fontsize=8, weight='bold',\n",
        "                bbox=dict(boxstyle='round,pad=0.2', facecolor=color, alpha=0.8))\n",
        "    if transition_point:\n",
        "        ax2.scatter([transition_point['time']], [transition_point['value']],\n",
        "                   color='red', s=150, marker='^', edgecolor='black', linewidth=2)\n",
        "        ax2.text(transition_point['time'], transition_point['value'] + 0.05,\n",
        "                f\"Peak Transition (5→2)\\n{transition_point['time']:.1f}s\",\n",
        "                ha='center', fontsize=8, weight='bold', color='red',\n",
        "                bbox=dict(boxstyle='round,pad=0.2', facecolor='red', alpha=0.8))\n",
        "    ax2.set_title('P25 Signal with Center Cropping', fontsize=12, weight='bold')\n",
        "    ax2.set_ylabel('P25 Values')\n",
        "    ax2.set_xlabel('Time (s)')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    journey_points = detected_points.copy()\n",
        "    if transition_point:\n",
        "        journey_points.append(transition_point)\n",
        "    if journey_points:\n",
        "        journey_times = [p.get('center_time', p.get('time', 0)) for p in journey_points]\n",
        "        detection_nums = list(range(1, len(journey_points) + 1))\n",
        "        all_colors = ['blue', 'orange', 'green', 'red', 'purple', 'brown']\n",
        "        point_colors = all_colors[:len(journey_points)]\n",
        "        ax3.scatter(journey_times, detection_nums, s=150, c=point_colors,\n",
        "                   edgecolor='black', linewidth=2)\n",
        "        ax3.plot(journey_times, detection_nums, 'k--', alpha=0.5, linewidth=2)\n",
        "        for time_point, det_num, point in zip(journey_times, detection_nums, journey_points):\n",
        "            if 'max_duration_allowed' in point:\n",
        "                crop_status = \"Cropped\" if point.get('was_cropped', False) else \"Original\"\n",
        "                info_text = f\"Det {det_num}\\n{time_point:.1f}s\\n{crop_status}\\n{point.get('duration', 0):.1f}s\"\n",
        "            else:\n",
        "                info_text = f\"Det {det_num}\\n{time_point:.1f}s\\nPeak (5→2)\"\n",
        "            ax3.text(time_point, det_num + 0.2, info_text,\n",
        "                    ha='center', fontsize=8, weight='bold')\n",
        "    ax3.set_title('Detection Timeline with Cropping Status', fontsize=12, weight='bold')\n",
        "    ax3.set_xlabel('Time (s)')\n",
        "    ax3.set_ylabel('Detection Number')\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "    table_data = []\n",
        "    for point in detected_points:\n",
        "        min_met = \"Yes\" if point.get('min_duration_met', False) else \"No\"\n",
        "        was_cropped = \"Yes\" if point.get('was_cropped', False) else \"No\"\n",
        "        original_dur = point.get('original_duration', point['duration'])\n",
        "        table_data.append([\n",
        "            point['name'],\n",
        "            f\"{point['center_time']:.1f}\",\n",
        "            f\"{point['duration']:.1f}\",\n",
        "            f\"{original_dur:.1f}\",\n",
        "            f\"{point['max_duration_allowed']:.1f}\",\n",
        "            was_cropped,\n",
        "            min_met\n",
        "        ])\n",
        "    if transition_point:\n",
        "        table_data.append(['Peak Transition (5→2)', f\"{transition_point['time']:.1f}\", 'N/A', 'N/A', 'N/A', 'N/A', 'N/A'])\n",
        "    if table_data:\n",
        "        table = ax4.table(cellText=table_data,\n",
        "                         colLabels=['Detection', 'Time (s)', 'Final Dur (s)', 'Original Dur (s)', 'Max Allowed (s)', 'Cropped', 'Valid'],\n",
        "                         cellLoc='center', loc='center', bbox=[0, 0, 1, 1])\n",
        "        table.auto_set_font_size(False)\n",
        "        table.set_fontsize(7)\n",
        "        table.scale(1, 2.8)\n",
        "        for i in range(len(table_data) + 1):\n",
        "            for j in range(7):\n",
        "                cell = table[(i, j)]\n",
        "                if i == 0:\n",
        "                    cell.set_facecolor('#4ECDC4')\n",
        "                    cell.set_text_props(weight='bold', color='white')\n",
        "                else:\n",
        "                    cell.set_facecolor('#F8F9FA')\n",
        "                cell.set_edgecolor('white')\n",
        "                cell.set_linewidth(2)\n",
        "    ax4.set_title('Detection Results with Center Cropping', fontsize=12, weight='bold')\n",
        "    ax4.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def analyze_case(case_file, dataset_type='training'):\n",
        "    \"\"\"Run stationary point analysis and summarize cropping actions.\"\"\"\n",
        "    df = load_data(case_file, dataset_type)\n",
        "    time_values, abs_acceleration, p25_signal = calculate_features(df)\n",
        "    detection_config = [\n",
        "        {\n",
        "            'name': 'Location 8',\n",
        "            'start': 0, 'end': 15,\n",
        "            'factor': 1.3,\n",
        "            'min_stationary_duration': 1.0,\n",
        "            'max_allowed_duration': 10.0\n",
        "        },\n",
        "        {\n",
        "            'name': 'Location 5',\n",
        "            'start': 20, 'end': 35,\n",
        "            'factor': 1.2,\n",
        "            'min_stationary_duration': 1.0,\n",
        "            'max_allowed_duration': 3.0\n",
        "        },\n",
        "        {\n",
        "            'name': 'Location 2',\n",
        "            'start': 35, 'end': 46,\n",
        "            'factor': 1.2,\n",
        "            'min_stationary_duration': 1.0,\n",
        "            'max_allowed_duration': 4\n",
        "        }\n",
        "    ]\n",
        "    detected_points = []\n",
        "    for config in detection_config:\n",
        "        result = detect_stationary_with_center_cropping(\n",
        "            p25_signal, time_values,\n",
        "            config['start'], config['end'],\n",
        "            config['factor'],\n",
        "            config['min_stationary_duration'],\n",
        "            config['max_allowed_duration']\n",
        "        )\n",
        "        if result:\n",
        "            result['name'] = config['name']\n",
        "            detected_points.append(result)\n",
        "    transition_point = None\n",
        "    location_5_point = None\n",
        "    location_2_point = None\n",
        "    for point in detected_points:\n",
        "        if 'Location 5' in point['name']:\n",
        "            location_5_point = point\n",
        "        elif 'Location 2' in point['name']:\n",
        "            location_2_point = point\n",
        "    if location_5_point is not None and location_2_point is not None:\n",
        "        search_start = location_5_point['end_time'] + 0.5\n",
        "        search_end = location_2_point['start_time'] - 0.5\n",
        "        if search_start < search_end:\n",
        "            transition_point = detect_transition_peak_between_5_and_2(\n",
        "                p25_signal, time_values, search_start, search_end\n",
        "            )\n",
        "    create_clean_visualization(time_values, abs_acceleration, p25_signal,\n",
        "                             detected_points, transition_point, case_file, dataset_type)\n",
        "    # Output detection summary for user\n",
        "    print(f\"\\nDetected {len(detected_points)} stationary points:\")\n",
        "    for point in detected_points:\n",
        "        crop_status = \"(Cropped)\" if point.get('was_cropped', False) else \"(Original)\"\n",
        "        original_dur = point.get('original_duration', point['duration'])\n",
        "        print(f\"  {point['name']}: {point['center_time']:.1f}s\")\n",
        "        print(f\"    Final Duration: {point['duration']:.1f}s {crop_status}\")\n",
        "        print(f\"    Original Duration: {original_dur:.1f}s\")\n",
        "        print(f\"    Max Allowed: {point['max_duration_allowed']:.1f}s\")\n",
        "    if transition_point:\n",
        "        print(f\"  Peak Transition (5→2): {transition_point['time']:.1f}s\")\n",
        "\n",
        "# Example usage\n",
        "analyze_case('G6_P5_case62.xls', 'training')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ASSIGNING SECTION MAPPING TO ALL THE FILES BASED ON THE USED THRESHOLDS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iW9eixgJ4egZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"PROCESSING ALL SENSOR 5 FILES WITH SECTION MAPPING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Setup paths\n",
        "training_data_path = '/content/drive/MyDrive/IMES/imes/data_files/Raw Data/Training Data'\n",
        "testing_data_path = '/content/drive/MyDrive/IMES/imes/data_files/Raw Data/Testing Data'\n",
        "output_base_path = '/content/drive/MyDrive/IMES/imes/processed'\n",
        "section_mapping_path = os.path.join(output_base_path, 'section mapping')\n",
        "os.makedirs(section_mapping_path, exist_ok=True)\n",
        "print(f\"Section mapping folder created: {section_mapping_path}\")\n",
        "\n",
        "# Load metadata\n",
        "training_metadata_path = os.path.join(output_base_path, 'Cleaned_Training_Info.csv')\n",
        "testing_metadata_path = os.path.join(output_base_path, 'Cleaned_Testing_Info.csv')\n",
        "df_training_clean = pd.read_csv(training_metadata_path)\n",
        "df_testing_clean = pd.read_csv(testing_metadata_path)\n",
        "\n",
        "# Filter Sensor 5 files\n",
        "sensor5_training = df_training_clean[df_training_clean['Sensor Position'] == 5].copy()\n",
        "sensor5_testing = df_testing_clean[df_testing_clean['Sensor Position'] == 5].copy()\n",
        "print(f\"Sensor 5 Training files: {len(sensor5_training)}\")\n",
        "print(f\"Sensor 5 Testing files: {len(sensor5_testing)}\")\n",
        "\n",
        "def calculate_features(df):\n",
        "    time_values = df['Time'].values\n",
        "    abs_acceleration = np.sqrt(df['ax']**2 + df['ay']**2 + df['az']**2)\n",
        "    p25_values = []\n",
        "    window_size = 200\n",
        "\n",
        "    for i in range(len(abs_acceleration)):\n",
        "        start_idx = max(0, i - window_size//2)\n",
        "        end_idx = min(len(abs_acceleration), i + window_size//2)\n",
        "        window_signal = abs_acceleration[start_idx:end_idx]\n",
        "        if len(window_signal) >= 10:\n",
        "            p25 = np.percentile(window_signal, 25)\n",
        "            p25_values.append(p25)\n",
        "        else:\n",
        "            p25_values.append(p25_values[-1] if p25_values else 0)\n",
        "    return time_values, abs_acceleration, np.array(p25_values)\n",
        "\n",
        "def detect_stationary_with_center_cropping(p25_signal, time_vals, start_time, end_time,\n",
        "                                          threshold_factor, min_stationary_duration,\n",
        "                                          max_allowed_duration):\n",
        "    mask = (time_vals >= start_time) & (time_vals <= end_time)\n",
        "    window_p25 = p25_signal[mask]\n",
        "    window_time = time_vals[mask]\n",
        "    if len(window_p25) < 20:\n",
        "        return None\n",
        "    baseline = np.percentile(window_p25, 25)\n",
        "    threshold = baseline * threshold_factor\n",
        "    stationary_mask = window_p25 <= threshold\n",
        "\n",
        "    best_start, best_end, best_duration = None, None, 0\n",
        "    current_start = None\n",
        "\n",
        "    for i, is_stationary in enumerate(stationary_mask):\n",
        "        if is_stationary and current_start is None:\n",
        "            current_start = i\n",
        "        elif not is_stationary and current_start is not None:\n",
        "            duration = window_time[i-1] - window_time[current_start]\n",
        "            if duration > best_duration and duration >= min_stationary_duration:\n",
        "                best_duration = duration\n",
        "                best_start = window_time[current_start]\n",
        "                best_end = window_time[i-1]\n",
        "            current_start = None\n",
        "\n",
        "    if current_start is not None:\n",
        "        duration = window_time[-1] - window_time[current_start]\n",
        "        if duration > best_duration and duration >= min_stationary_duration:\n",
        "            best_duration = duration\n",
        "            best_start = window_time[current_start]\n",
        "            best_end = window_time[-1]\n",
        "\n",
        "    if best_start is not None:\n",
        "        original_center = (best_start + best_end) / 2\n",
        "        original_duration = best_duration\n",
        "        if best_duration > max_allowed_duration:\n",
        "            half_allowed = max_allowed_duration / 2\n",
        "            new_start = original_center - half_allowed\n",
        "            new_end = original_center + half_allowed\n",
        "            new_start = max(new_start, best_start)\n",
        "            new_end = min(new_end, best_end)\n",
        "            best_start = new_start\n",
        "            best_end = new_end\n",
        "            best_duration = new_end - new_start\n",
        "\n",
        "        center_time = (best_start + best_end) / 2\n",
        "        final_mask = (window_time >= best_start) & (window_time <= best_end)\n",
        "        avg_p25 = np.mean(window_p25[final_mask]) if np.any(final_mask) else baseline\n",
        "\n",
        "        return {\n",
        "            'start_time': best_start, 'end_time': best_end, 'center_time': center_time,\n",
        "            'duration': best_duration, 'avg_p25': avg_p25, 'threshold': threshold,\n",
        "            'min_duration_met': best_duration >= min_stationary_duration,\n",
        "            'was_cropped': original_duration > max_allowed_duration,\n",
        "            'original_duration': original_duration,\n",
        "            'max_duration_allowed': max_allowed_duration\n",
        "        }\n",
        "    return None\n",
        "\n",
        "def detect_transition_peak_between_5_and_2(p25_signal, time_vals, search_start, search_end):\n",
        "    mask = (time_vals >= search_start) & (time_vals <= search_end)\n",
        "    if not np.any(mask):\n",
        "        return None\n",
        "    transition_p25 = p25_signal[mask]\n",
        "    transition_time = time_vals[mask]\n",
        "    if len(transition_p25) < 5:\n",
        "        return None\n",
        "    max_idx = np.argmax(transition_p25)\n",
        "    return {\n",
        "        'name': 'Peak Transition (5→2)',\n",
        "        'time': transition_time[max_idx],\n",
        "        'value': transition_p25[max_idx]\n",
        "    }\n",
        "\n",
        "def add_section_column(df, detected_points, transition_point):\n",
        "    time_values = df['Time'].values\n",
        "    sections = np.full(len(time_values), 8)\n",
        "    location_8_point = location_5_point = location_2_point = None\n",
        "    for point in detected_points:\n",
        "        if 'Location 8' in point['name']:\n",
        "            location_8_point = point\n",
        "        elif 'Location 5' in point['name']:\n",
        "            location_5_point = point\n",
        "        elif 'Location 2' in point['name']:\n",
        "            location_2_point = point\n",
        "    if location_8_point:\n",
        "        mask_start_to_8 = time_values <= location_8_point['end_time']\n",
        "        sections[mask_start_to_8] = 8\n",
        "    if location_8_point and location_5_point:\n",
        "        mask_6 = ((time_values > location_8_point['end_time']) &\n",
        "                  (time_values < location_5_point['start_time']))\n",
        "        sections[mask_6] = 6\n",
        "    if location_5_point:\n",
        "        mask_5 = ((time_values >= location_5_point['start_time']) &\n",
        "                  (time_values <= location_5_point['end_time']))\n",
        "        sections[mask_5] = 5\n",
        "    if location_5_point and transition_point:\n",
        "        mask_4 = ((time_values > location_5_point['end_time']) &\n",
        "                  (time_values <= transition_point['time']))\n",
        "        sections[mask_4] = 4\n",
        "    if transition_point and location_2_point:\n",
        "        mask_3 = ((time_values > transition_point['time']) &\n",
        "                  (time_values < location_2_point['start_time']))\n",
        "        sections[mask_3] = 3\n",
        "    if location_2_point:\n",
        "        mask_2 = time_values >= location_2_point['start_time']\n",
        "        sections[mask_2] = 2\n",
        "    if not transition_point and location_5_point and location_2_point:\n",
        "        mid_time = (location_5_point['end_time'] + location_2_point['start_time']) / 2\n",
        "        mask_4 = ((time_values > location_5_point['end_time']) &\n",
        "                  (time_values <= mid_time))\n",
        "        sections[mask_4] = 4\n",
        "        mask_3 = ((time_values > mid_time) &\n",
        "                  (time_values < location_2_point['start_time']))\n",
        "        sections[mask_3] = 3\n",
        "    return sections.astype(int)\n",
        "\n",
        "def process_single_sensor5_file_with_sections(case_file, case_id, dataset_type='training'):\n",
        "    try:\n",
        "        if dataset_type.lower() == 'training':\n",
        "            file_path = os.path.join(training_data_path, case_file)\n",
        "        else:\n",
        "            file_path = os.path.join(testing_data_path, case_file)\n",
        "        df = pd.read_excel(file_path, engine='xlrd')\n",
        "        df.columns = ['Time', 'ax', 'ay', 'az', 'a_abs']\n",
        "        time_values, abs_acceleration, p25_signal = calculate_features(df)\n",
        "        detection_config = [\n",
        "            {'name': 'Location 8', 'start': 0, 'end': 15, 'factor': 1.3, 'min_stationary_duration': 1.0, 'max_allowed_duration': 10.0},\n",
        "            {'name': 'Location 5', 'start': 20, 'end': 35, 'factor': 1.2, 'min_stationary_duration': 1.0, 'max_allowed_duration': 3.0},\n",
        "            {'name': 'Location 2', 'start': 35, 'end': 46, 'factor': 1.2, 'min_stationary_duration': 1.0, 'max_allowed_duration': 5}\n",
        "        ]\n",
        "        detected_points = []\n",
        "        for config in detection_config:\n",
        "            result = detect_stationary_with_center_cropping(\n",
        "                p25_signal, time_values,\n",
        "                config['start'], config['end'],\n",
        "                config['factor'],\n",
        "                config['min_stationary_duration'],\n",
        "                config['max_allowed_duration']\n",
        "            )\n",
        "            if result:\n",
        "                result['name'] = config['name']\n",
        "                detected_points.append(result)\n",
        "        transition_point = None\n",
        "        location_5_point = location_2_point = None\n",
        "        for point in detected_points:\n",
        "            if 'Location 5' in point['name']:\n",
        "                location_5_point = point\n",
        "            elif 'Location 2' in point['name']:\n",
        "                location_2_point = point\n",
        "        if location_5_point and location_2_point:\n",
        "            search_start = location_5_point['end_time'] + 0.5\n",
        "            search_end = location_2_point['start_time'] - 0.5\n",
        "            if search_start < search_end:\n",
        "                transition_point = detect_transition_peak_between_5_and_2(\n",
        "                    p25_signal, time_values, search_start, search_end\n",
        "                )\n",
        "        sections = add_section_column(df, detected_points, transition_point)\n",
        "        df['section'] = sections\n",
        "        df['case_id'] = case_id\n",
        "        df['dataset'] = dataset_type\n",
        "        df['filename'] = case_file\n",
        "        df['P25_acceleration'] = p25_signal\n",
        "        output_filename = f\"sensor5_case{case_id}.csv\"\n",
        "        output_path = os.path.join(section_mapping_path, output_filename)\n",
        "        df.to_csv(output_path, index=False)\n",
        "        return {\n",
        "            'case_id': case_id,\n",
        "            'dataset': dataset_type,\n",
        "            'original_filename': case_file,\n",
        "            'output_filename': output_filename,\n",
        "            'output_path': output_path,\n",
        "            'success': True,\n",
        "            'samples': len(df),\n",
        "            'sections': sorted(df['section'].unique()),\n",
        "            'detections': len(detected_points),\n",
        "            'transition_detected': transition_point is not None,\n",
        "            'processing_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            'case_id': case_id,\n",
        "            'dataset': dataset_type,\n",
        "            'original_filename': case_file,\n",
        "            'success': False,\n",
        "            'error': str(e),\n",
        "            'processing_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "        }\n",
        "\n",
        "print(\"Processing all Sensor 5 files...\")\n",
        "all_results = []\n",
        "for idx, row in tqdm(sensor5_training.iterrows(), total=len(sensor5_training), desc=\"Training\"):\n",
        "    case_file = row['File Name']\n",
        "    case_id = row['Case_ID']\n",
        "    result = process_single_sensor5_file_with_sections(case_file, case_id, 'training')\n",
        "    all_results.append(result)\n",
        "\n",
        "for idx, row in tqdm(sensor5_testing.iterrows(), total=len(sensor5_testing), desc=\"Testing\"):\n",
        "    case_file = row['File Name']\n",
        "    case_id = row['Case_ID']\n",
        "    result = process_single_sensor5_file_with_sections(case_file, case_id, 'testing')\n",
        "    all_results.append(result)\n",
        "\n",
        "results_df = pd.DataFrame(all_results)\n",
        "summary_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "summary_filename = f'sensor5_section_mapping_summary_{summary_timestamp}.csv'\n",
        "summary_path = os.path.join(section_mapping_path, summary_filename)\n",
        "results_df.to_csv(summary_path, index=False)\n",
        "\n",
        "successful_results = [r for r in all_results if r['success']]\n",
        "failed_results = [r for r in all_results if not r['success']]\n",
        "\n",
        "print(\"\\nSection mapping processing complete.\")\n",
        "print(f\"Output folder: {section_mapping_path}\")\n",
        "print(f\"Total files processed: {len(all_results)}\")\n",
        "print(f\"Successful: {len(successful_results)}\")\n",
        "print(f\"Failed: {len(failed_results)}\")\n",
        "print(f\"Summary report: {summary_filename}\")\n",
        "\n",
        "if successful_results:\n",
        "    total_samples = sum(r['samples'] for r in successful_results)\n",
        "    print(f\"Total samples: {total_samples}\")\n",
        "    section_counts = {}\n",
        "    for result in successful_results:\n",
        "        for section in result['sections']:\n",
        "            section_counts[section] = section_counts.get(section, 0) + 1\n",
        "    print(\"Sections detected across files:\")\n",
        "    for section in sorted(section_counts.keys()):\n",
        "        print(f\"  Section {section}: Found in {section_counts[section]} files\")\n",
        "    print(\"Example output files:\")\n",
        "    for i, result in enumerate(successful_results[:5], 1):\n",
        "        print(f\"  {i}. {result['output_filename']}\")\n",
        "        print(f\"     Original: {result['original_filename']}\")\n",
        "        print(f\"     Case: {result['case_id']}, Dataset: {result['dataset']}\")\n",
        "        print(f\"     Samples: {result['samples']}, Sections: {result['sections']}\")\n",
        "\n",
        "if failed_results:\n",
        "    print(\"Failed files:\")\n",
        "    for result in failed_results:\n",
        "        print(f\"  {result['original_filename']}: {result['error']}\")\n",
        "\n",
        "output_files = [f for f in os.listdir(section_mapping_path) if f.endswith('.csv')]\n",
        "print(f\"Section mapping folder contents:\")\n",
        "print(f\"  Folder: {section_mapping_path}\")\n",
        "print(f\"  Total CSV files: {len(output_files)}\")\n",
        "print(f\"  Summary file: {summary_filename}\")\n",
        "print(f\"  Data files: {len(output_files) - 1}\")\n",
        "\n",
        "print(f\"All Sensor 5 files with section mapping saved to: {section_mapping_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SECTIONWISE FEATURE EXTRACION FOR FREQUENCY DEFECT LOCATION DETECTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRODeMBz5Lm5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "from scipy.fft import fft, fftfreq\n",
        "from scipy.signal import butter, filtfilt\n",
        "from scipy import stats\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Paths and Setup\n",
        "output_base_path = '/content/drive/MyDrive/IMES/imes/processed'\n",
        "section_mapping_path = os.path.join(output_base_path, 'section mapping')\n",
        "frequency_analysis_path = os.path.join(output_base_path, 'Frequency_Analysis_Simple.csv')\n",
        "frequency_df = pd.read_csv(frequency_analysis_path)\n",
        "section_files = glob.glob(os.path.join(section_mapping_path, \"sensor5_case*.csv\"))\n",
        "\n",
        "def extract_comprehensive_section_features(df_section, section_id, sampling_freq=100):\n",
        "    features = {'section_id': section_id}\n",
        "\n",
        "    if len(df_section) < 10:\n",
        "        return None\n",
        "\n",
        "    ax = df_section['ax'].values\n",
        "    ay = df_section['ay'].values\n",
        "    az = df_section['az'].values\n",
        "    abs_acc = np.sqrt(ax**2 + ay**2 + az**2)\n",
        "\n",
        "    # Sampling frequency from time data if available\n",
        "    if 'Time' in df_section.columns and len(df_section) > 1:\n",
        "        time_intervals = np.diff(df_section['Time'].values)\n",
        "        sampling_freq = 1.0 / np.mean(time_intervals) if np.mean(time_intervals) > 0 else 100\n",
        "\n",
        "    for axis_name, axis_data in [('ax', ax), ('ay', ay), ('az', az), ('abs', abs_acc)]:\n",
        "        features[f'{axis_name}_mean'] = np.mean(axis_data)\n",
        "        features[f'{axis_name}_std'] = np.std(axis_data)\n",
        "        features[f'{axis_name}_var'] = np.var(axis_data)\n",
        "        features[f'{axis_name}_rms'] = np.sqrt(np.mean(axis_data**2))\n",
        "        features[f'{axis_name}_peak_to_peak'] = np.max(axis_data) - np.min(axis_data)\n",
        "        features[f'{axis_name}_mean_abs_dev'] = np.mean(np.abs(axis_data - np.mean(axis_data)))\n",
        "\n",
        "        features[f'{axis_name}_skewness'] = stats.skew(axis_data)\n",
        "        features[f'{axis_name}_kurtosis'] = stats.kurtosis(axis_data)\n",
        "        features[f'{axis_name}_percentile_25'] = np.percentile(axis_data, 25)\n",
        "        features[f'{axis_name}_percentile_50'] = np.percentile(axis_data, 50)\n",
        "        features[f'{axis_name}_percentile_75'] = np.percentile(axis_data, 75)\n",
        "        features[f'{axis_name}_percentile_90'] = np.percentile(axis_data, 90)\n",
        "        features[f'{axis_name}_iqr'] = np.percentile(axis_data, 75) - np.percentile(axis_data, 25)\n",
        "\n",
        "        features[f'{axis_name}_energy'] = np.sum(axis_data**2)\n",
        "        features[f'{axis_name}_log_energy'] = np.log(np.sum(axis_data**2) + 1e-8)\n",
        "        zero_crossings = np.sum(np.diff(np.signbit(axis_data - np.mean(axis_data))))\n",
        "        features[f'{axis_name}_zero_crossings'] = zero_crossings\n",
        "\n",
        "        if len(axis_data) > 10:\n",
        "            try:\n",
        "                nyquist = 0.5 * sampling_freq\n",
        "                lowcut = min(25.0, nyquist * 0.4)\n",
        "                highcut = min(65.0, nyquist * 0.9)\n",
        "                if lowcut < highcut and lowcut > 0:\n",
        "                    low = lowcut / nyquist\n",
        "                    high = highcut / nyquist\n",
        "                    low = max(0.01, min(low, 0.98))\n",
        "                    high = max(low + 0.01, min(high, 0.99))\n",
        "                    b, a = butter(4, [low, high], btype='band')\n",
        "                    filtered_data = filtfilt(b, a, axis_data)\n",
        "                else:\n",
        "                    filtered_data = axis_data\n",
        "                fft_vals = np.abs(fft(filtered_data))[:len(filtered_data)//2]\n",
        "                freqs = fftfreq(len(filtered_data), 1/sampling_freq)[:len(filtered_data)//2]\n",
        "                if len(fft_vals) > 0 and np.sum(fft_vals) > 0:\n",
        "                    dominant_idx = np.argmax(fft_vals)\n",
        "                    features[f'{axis_name}_dominant_freq'] = freqs[dominant_idx] if dominant_idx < len(freqs) else 0\n",
        "                    features[f'{axis_name}_dominant_magnitude'] = fft_vals[dominant_idx]\n",
        "                    features[f'{axis_name}_spectral_centroid'] = np.sum(freqs * fft_vals) / np.sum(fft_vals)\n",
        "                    features[f'{axis_name}_spectral_energy'] = np.sum(fft_vals**2)\n",
        "                    freq_25_35 = (freqs >= 25) & (freqs <= 35)\n",
        "                    freq_35_45 = (freqs >= 35) & (freqs <= 45)\n",
        "                    freq_45_55 = (freqs >= 45) & (freqs <= 55)\n",
        "                    freq_55_65 = (freqs >= 55) & (freqs <= 65)\n",
        "                    features[f'{axis_name}_power_25_35'] = np.sum(fft_vals[freq_25_35]**2)\n",
        "                    features[f'{axis_name}_power_35_45'] = np.sum(fft_vals[freq_35_45]**2)\n",
        "                    features[f'{axis_name}_power_45_55'] = np.sum(fft_vals[freq_45_55]**2)\n",
        "                    features[f'{axis_name}_power_55_65'] = np.sum(fft_vals[freq_55_65]**2)\n",
        "                    features[f'{axis_name}_mean_magnitude'] = np.mean(fft_vals)\n",
        "                    features[f'{axis_name}_max_magnitude'] = np.max(fft_vals)\n",
        "                    cumsum_fft = np.cumsum(fft_vals**2)\n",
        "                    total_energy = cumsum_fft[-1]\n",
        "                    rolloff_idx = np.where(cumsum_fft >= 0.85 * total_energy)[0]\n",
        "                    features[f'{axis_name}_spectral_rolloff'] = freqs[rolloff_idx[0]] if len(rolloff_idx) > 0 else 0\n",
        "                else:\n",
        "                    for feat in ['dominant_freq', 'dominant_magnitude', 'spectral_centroid',\n",
        "                                 'spectral_energy', 'power_25_35', 'power_35_45', 'power_45_55',\n",
        "                                 'power_55_65', 'mean_magnitude', 'max_magnitude', 'spectral_rolloff']:\n",
        "                        features[f'{axis_name}_{feat}'] = 0\n",
        "            except Exception:\n",
        "                for feat in ['dominant_freq', 'dominant_magnitude', 'spectral_centroid',\n",
        "                             'spectral_energy', 'power_25_35', 'power_35_45', 'power_45_55',\n",
        "                             'power_55_65', 'mean_magnitude', 'max_magnitude', 'spectral_rolloff']:\n",
        "                    features[f'{axis_name}_{feat}'] = 0\n",
        "        else:\n",
        "            for feat in ['dominant_freq', 'dominant_magnitude', 'spectral_centroid',\n",
        "                         'spectral_energy', 'power_25_35', 'power_35_45', 'power_45_55',\n",
        "                         'power_55_65', 'mean_magnitude', 'max_magnitude', 'spectral_rolloff']:\n",
        "                features[f'{axis_name}_{feat}'] = 0\n",
        "\n",
        "    features['section_duration'] = len(df_section) / sampling_freq\n",
        "    features['section_samples'] = len(df_section)\n",
        "    features['sampling_frequency'] = sampling_freq\n",
        "\n",
        "    return features\n",
        "\n",
        "def process_section_mapped_files():\n",
        "    all_section_features = []\n",
        "    for file_path in tqdm(section_files, desc=\"Extracting features\"):\n",
        "        try:\n",
        "            df = pd.read_csv(file_path)\n",
        "            filename = os.path.basename(file_path)\n",
        "            case_id = filename.replace('sensor5_case', '').replace('.csv', '')\n",
        "            freq_info = frequency_df[frequency_df['Case_ID'] == case_id].iloc[0] if len(frequency_df[frequency_df['Case_ID'] == case_id]) > 0 else None\n",
        "            has_frequency_defect = freq_info['Has_Frequency_Defect'] if freq_info is not None else 0\n",
        "            defect_location = freq_info['Ground_Truth_Freq_Location'] if freq_info is not None else None\n",
        "            defect_frequency = freq_info['Ground_Truth_Freq'] if freq_info is not None else None\n",
        "            sections_in_file = sorted(df['section'].unique())\n",
        "            for section_id in sections_in_file:\n",
        "                section_data = df[df['section'] == section_id].copy()\n",
        "                features = extract_comprehensive_section_features(section_data, section_id)\n",
        "                if features is not None:\n",
        "                    features['case_id'] = case_id\n",
        "                    features['dataset'] = df['dataset'].iloc[0] if 'dataset' in df.columns else 'unknown'\n",
        "                    features['has_frequency_defect'] = has_frequency_defect\n",
        "                    features['defect_location'] = defect_location if defect_location is not None else -1\n",
        "                    features['defect_frequency'] = defect_frequency if defect_frequency is not None else -1\n",
        "                    if has_frequency_defect and defect_location is not None and not pd.isna(defect_location):\n",
        "                        features['section_has_defect'] = 1 if section_id == int(defect_location) else 0\n",
        "                    else:\n",
        "                        features['section_has_defect'] = 0\n",
        "                    all_section_features.append(features)\n",
        "        except Exception:\n",
        "            continue\n",
        "    return all_section_features\n",
        "\n",
        "section_features_list = process_section_mapped_files()\n",
        "section_features_df = pd.DataFrame(section_features_list)\n",
        "\n",
        "# Section-wise features are now ready for the frequency defect location model.\n",
        "features_output_path = os.path.join(output_base_path, 'section_wise_features_for_location_prediction.csv')\n",
        "section_features_df.to_csv(features_output_path, index=False)\n",
        "\n",
        "# Preview key results\n",
        "print(f\"Total sections processed: {len(section_features_df):,}\")\n",
        "print(f\"Unique cases: {section_features_df['case_id'].nunique()}\")\n",
        "print(f\"Features per section: {len([col for col in section_features_df.columns if col not in ['case_id', 'dataset', 'section_id', 'has_frequency_defect', 'defect_location', 'defect_frequency', 'section_has_defect']])}\")\n",
        "print(section_features_df[['case_id', 'section_id', 'section_has_defect', 'defect_location', 'ax_mean', 'ay_dominant_freq', 'az_power_35_45']].head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FREQUENCY DEFECT LOCATION PREDICTION MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgj3DItB5vdA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "from sklearn.metrics import precision_recall_curve, roc_curve\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import os\n",
        "import joblib\n",
        "import json\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load the section-wise features dataset\n",
        "output_base_path = '/content/drive/MyDrive/IMES/imes/processed'\n",
        "features_path = os.path.join(output_base_path, 'section_wise_features_for_location_prediction.csv')\n",
        "\n",
        "df_features = pd.read_csv(features_path)\n",
        "\n",
        "# Prepare features and target\n",
        "feature_cols = [col for col in df_features.columns if col not in [\n",
        "    'case_id', 'dataset', 'section_id', 'has_frequency_defect',\n",
        "    'defect_location', 'defect_frequency', 'section_has_defect', 'filename'\n",
        "]]\n",
        "\n",
        "# Handle missing values\n",
        "df_features[feature_cols] = df_features[feature_cols].fillna(0)\n",
        "feature_vars = df_features[feature_cols].var()\n",
        "zero_var_features = feature_vars[feature_vars == 0].index.tolist()\n",
        "if zero_var_features:\n",
        "    feature_cols = [col for col in feature_cols if col not in zero_var_features]\n",
        "\n",
        "X = df_features[feature_cols].values\n",
        "y = df_features['section_has_defect'].values\n",
        "case_ids = df_features['case_id'].values\n",
        "section_ids = df_features['section_id'].values\n",
        "\n",
        "# TRAIN-TEST SPLIT BY CASE ID (NO DATA LEAKAGE)\n",
        "case_info = df_features.groupby('case_id').agg({\n",
        "    'has_frequency_defect': 'first',\n",
        "    'section_has_defect': 'sum'\n",
        "}).reset_index()\n",
        "cases_with_defects = case_info[case_info['has_frequency_defect'] == 1]['case_id'].values\n",
        "cases_without_defects = case_info[case_info['has_frequency_defect'] == 0]['case_id'].values\n",
        "train_cases_with, test_cases_with = train_test_split(\n",
        "    cases_with_defects, test_size=0.2, random_state=42\n",
        ")\n",
        "train_cases_without, test_cases_without = train_test_split(\n",
        "    cases_without_defects, test_size=0.2, random_state=42\n",
        ")\n",
        "train_cases = np.concatenate([train_cases_with, train_cases_without])\n",
        "test_cases = np.concatenate([test_cases_with, test_cases_without])\n",
        "train_mask = np.isin(case_ids, train_cases)\n",
        "test_mask = np.isin(case_ids, test_cases)\n",
        "X_train, X_test = X[train_mask], X[test_mask]\n",
        "y_train, y_test = y[train_mask], y[test_mask]\n",
        "case_ids_train, case_ids_test = case_ids[train_mask], case_ids[test_mask]\n",
        "section_ids_train, section_ids_test = section_ids[train_mask], section_ids[test_mask]\n",
        "\n",
        "# FEATURE SCALING\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# MODEL TRAINING WITH CLASS IMBALANCE HANDLING\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
        "\n",
        "models = {\n",
        "    'RandomForest': RandomForestClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=10,\n",
        "        class_weight='balanced',\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    ),\n",
        "    'GradientBoosting': GradientBoostingClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=6,\n",
        "        learning_rate=0.1,\n",
        "        random_state=42\n",
        "    ),\n",
        "    'LogisticRegression': LogisticRegression(\n",
        "        class_weight='balanced',\n",
        "        random_state=42,\n",
        "        max_iter=1000\n",
        "    ),\n",
        "    'SVM': SVC(\n",
        "        class_weight='balanced',\n",
        "        probability=True,\n",
        "        random_state=42\n",
        "    )\n",
        "}\n",
        "\n",
        "results = {}\n",
        "trained_models = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    if name == 'GradientBoosting':\n",
        "        smote = SMOTE(random_state=42)\n",
        "        X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)\n",
        "        model.fit(X_train_balanced, y_train_balanced)\n",
        "    else:\n",
        "        model.fit(X_train_scaled, y_train)\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
        "    accuracy = (y_pred == y_test).mean()\n",
        "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
        "    results[name] = {\n",
        "        'accuracy': accuracy,\n",
        "        'auc': auc_score,\n",
        "        'predictions': y_pred,\n",
        "        'probabilities': y_pred_proba\n",
        "    }\n",
        "    trained_models[name] = model\n",
        "\n",
        "# SELECT BEST MODEL AND DETAILED EVALUATION\n",
        "best_model_name = max(results.keys(), key=lambda x: results[x]['auc'])\n",
        "best_model = trained_models[best_model_name]\n",
        "best_predictions = results[best_model_name]['predictions']\n",
        "best_probabilities = results[best_model_name]['probabilities']\n",
        "\n",
        "cm = confusion_matrix(y_test, best_predictions)\n",
        "section_performance = []\n",
        "for section in sorted(np.unique(section_ids_test)):\n",
        "    section_mask = section_ids_test == section\n",
        "    if np.sum(section_mask) > 0:\n",
        "        section_y_true = y_test[section_mask]\n",
        "        section_y_pred = best_predictions[section_mask]\n",
        "        section_y_proba = best_probabilities[section_mask]\n",
        "        section_accuracy = (section_y_true == section_y_pred).mean()\n",
        "        if len(np.unique(section_y_true)) > 1:\n",
        "            section_auc = roc_auc_score(section_y_true, section_y_proba)\n",
        "        else:\n",
        "            section_auc = np.nan\n",
        "        total_samples = len(section_y_true)\n",
        "        defective_samples = np.sum(section_y_true)\n",
        "        section_performance.append({\n",
        "            'section': section,\n",
        "            'accuracy': section_accuracy,\n",
        "            'auc': section_auc,\n",
        "            'total_samples': total_samples,\n",
        "            'defective_samples': defective_samples\n",
        "        })\n",
        "\n",
        "# FEATURE IMPORTANCE ANALYSIS\n",
        "if hasattr(best_model, 'feature_importances_'):\n",
        "    importances = best_model.feature_importances_\n",
        "    feature_importance_df = pd.DataFrame({\n",
        "        'feature': feature_cols,\n",
        "        'importance': importances\n",
        "    }).sort_values('importance', ascending=False)\n",
        "    categories = {\n",
        "        'Time Domain': ['mean', 'std', 'var', 'rms', 'peak_to_peak', 'mean_abs_dev'],\n",
        "        'Statistical': ['skewness', 'kurtosis', 'percentile', 'iqr'],\n",
        "        'Frequency': ['dominant_freq', 'spectral', 'power_'],\n",
        "        'Energy': ['energy', 'log_energy'],\n",
        "        'Motion': ['zero_crossings'],\n",
        "        'Context': ['duration', 'samples', 'frequency']\n",
        "    }\n",
        "    for category, keywords in categories.items():\n",
        "        category_features = [f for f in feature_cols if any(kw in f for kw in keywords)]\n",
        "        if category_features:\n",
        "            category_importance = feature_importance_df[\n",
        "                feature_importance_df['feature'].isin(category_features)\n",
        "            ]['importance'].sum()\n",
        "\n",
        "# CASE-LEVEL ANALYSIS\n",
        "case_analysis = []\n",
        "for case_id in np.unique(case_ids_test):\n",
        "    case_mask = case_ids_test == case_id\n",
        "    case_sections = section_ids_test[case_mask]\n",
        "    case_true = y_test[case_mask]\n",
        "    case_pred = best_predictions[case_mask]\n",
        "    case_proba = best_probabilities[case_mask]\n",
        "    true_defective_sections = case_sections[case_true == 1]\n",
        "    pred_defective_sections = case_sections[case_pred == 1]\n",
        "    max_proba_idx = np.argmax(case_proba)\n",
        "    predicted_location = case_sections[max_proba_idx]\n",
        "    max_probability = case_proba[max_proba_idx]\n",
        "    if len(true_defective_sections) > 0:\n",
        "        has_defect = True\n",
        "        true_location = true_defective_sections[0]\n",
        "        location_correct = predicted_location in true_defective_sections\n",
        "    else:\n",
        "        has_defect = False\n",
        "        true_location = None\n",
        "        location_correct = False\n",
        "    case_analysis.append({\n",
        "        'case_id': case_id,\n",
        "        'has_defect': has_defect,\n",
        "        'true_location': true_location,\n",
        "        'predicted_location': predicted_location,\n",
        "        'max_probability': max_probability,\n",
        "        'location_correct': location_correct,\n",
        "        'all_probabilities': dict(zip(case_sections, case_proba))\n",
        "    })\n",
        "cases_with_defects_test = [ca for ca in case_analysis if ca['has_defect']]\n",
        "location_accuracy = np.mean([ca['location_correct'] for ca in cases_with_defects_test])\n",
        "\n",
        "# SAVE MODEL AND RESULTS\n",
        "model_path = os.path.join(output_base_path, 'frequency_defect_location_model.pkl')\n",
        "scaler_path = os.path.join(output_base_path, 'frequency_defect_location_scaler.pkl')\n",
        "joblib.dump(best_model, model_path)\n",
        "joblib.dump(scaler, scaler_path)\n",
        "feature_names_path = os.path.join(output_base_path, 'frequency_defect_location_features.txt')\n",
        "with open(feature_names_path, 'w') as f:\n",
        "    for feature in feature_cols:\n",
        "        f.write(f\"{feature}\\n\")\n",
        "results_summary = {\n",
        "    'model_name': best_model_name,\n",
        "    'overall_accuracy': results[best_model_name]['accuracy'],\n",
        "    'overall_auc': results[best_model_name]['auc'],\n",
        "    'location_accuracy': location_accuracy,\n",
        "    'total_features': len(feature_cols),\n",
        "    'training_samples': len(X_train),\n",
        "    'testing_samples': len(X_test),\n",
        "    'class_distribution_train': np.bincount(y_train).tolist(),\n",
        "    'class_distribution_test': np.bincount(y_test).tolist()\n",
        "}\n",
        "results_path = os.path.join(output_base_path, 'frequency_defect_location_results.json')\n",
        "with open(results_path, 'w') as f:\n",
        "    json.dump(results_summary, f, indent=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ip3U-Lzo6LTs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "# --- Configuration ---\n",
        "output_base_path = '/content/drive/MyDrive/IMES/imes/processed'\n",
        "model_path = os.path.join(output_base_path, 'frequency_defect_location_model.pkl')\n",
        "scaler_path = os.path.join(output_base_path, 'frequency_defect_location_scaler.pkl')\n",
        "features_path = os.path.join(output_base_path, 'section_wise_features_for_location_prediction.csv')\n",
        "feature_names_path = os.path.join(output_base_path, 'frequency_defect_location_features.txt')\n",
        "metadata_path = os.path.join(output_base_path, 'Combined_Metadata.csv')\n",
        "\n",
        "# --- Load Model and Data ---\n",
        "print(\"=\"*80)\n",
        "print(\"PREDICTING DEFECT LOCATION FOR SPECIFIC CASES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "try:\n",
        "    model = joblib.load(model_path)\n",
        "    scaler = joblib.load(scaler_path)\n",
        "    with open(feature_names_path, 'r') as f:\n",
        "        feature_cols = [line.strip() for line in f.readlines()]\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Error: Could not find a required file: {e.filename}\")\n",
        "    exit()\n",
        "\n",
        "df_features = pd.read_csv(features_path)\n",
        "df_metadata = pd.read_csv(metadata_path)\n",
        "\n",
        "# --- Prepare Data for Prediction ---\n",
        "target_case_ids_str = ['1', '2', '3', '4', '5']\n",
        "ground_truth = {}\n",
        "if 'Defect Location' in df_metadata.columns:\n",
        "    df_metadata['Case_ID'] = df_metadata['Case_ID'].astype(str)\n",
        "    df_metadata['Defect Location'] = pd.to_numeric(df_metadata['Defect Location'], errors='coerce').astype('Int64')\n",
        "    gt_data = df_metadata[df_metadata['Case_ID'].isin(target_case_ids_str)].groupby('Case_ID').first()\n",
        "    for case_id in target_case_ids_str:\n",
        "        if case_id in gt_data.index and pd.notna(gt_data.loc[case_id, 'Defect Location']):\n",
        "            ground_truth[case_id] = gt_data.loc[case_id, 'Defect Location']\n",
        "        else:\n",
        "            ground_truth[case_id] = 'None'\n",
        "else:\n",
        "    for case_id in target_case_ids_str:\n",
        "        ground_truth[case_id] = 'N/A'\n",
        "\n",
        "df_features['case_id'] = df_features['case_id'].astype(str)\n",
        "prediction_data = df_features[df_features['case_id'].isin(target_case_ids_str)].copy()\n",
        "\n",
        "if prediction_data.empty:\n",
        "    print(f\"Error: No data found for Case IDs {target_case_ids_str} in the features file.\")\n",
        "    exit()\n",
        "\n",
        "prediction_data[feature_cols] = prediction_data[feature_cols].fillna(0)\n",
        "X_predict = prediction_data[feature_cols].values\n",
        "X_predict_scaled = scaler.transform(X_predict)\n",
        "\n",
        "# --- Make Predictions ---\n",
        "probabilities = model.predict_proba(X_predict_scaled)[:, 1]\n",
        "prediction_data['defect_probability'] = probabilities\n",
        "\n",
        "# --- Aggregate and Display Results ---\n",
        "results_list = []\n",
        "for case_id in sorted(target_case_ids_str):\n",
        "    case_data = prediction_data[prediction_data['case_id'] == case_id]\n",
        "    if case_data.empty:\n",
        "        continue\n",
        "    best_prediction_row = case_data.loc[case_data['defect_probability'].idxmax()]\n",
        "    predicted_location = int(best_prediction_row['section_id'])\n",
        "    max_confidence = best_prediction_row['defect_probability']\n",
        "    true_location = ground_truth.get(case_id, 'N/A')\n",
        "    results_list.append({\n",
        "        'Case ID': case_id,\n",
        "        'True Defect Location': true_location,\n",
        "        'Predicted Defect Location': predicted_location,\n",
        "        'Confidence': f\"{max_confidence:.2%}\"\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(results_list)\n",
        "print(results_df.to_string(index=False))\n",
        "print(\"=\"*60)\n",
        "print(\"Prediction process finished.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MULTI OUTPUT CNN MODEL FOR DAMPER AND INCLINATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RuVLGOVp6yZG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from scipy.signal import butter, lfilter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import f1_score, confusion_matrix, classification_report, accuracy_score, precision_score, recall_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"CNN DEFECT DETECTION WITH DYNAMIC SECTIONS & PERFORMANCE METRICS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# --- MANUAL FREQUENCY INPUT FOR TEST CASES ---\n",
        "MANUAL_TEST_FREQUENCY = {\n",
        "    '1': {'frequency_value': 35, 'frequency_location': 6},\n",
        "    '2': {'frequency_value': 0, 'frequency_location': 0},\n",
        "    '3': {'frequency_value': 40, 'frequency_location': 3},\n",
        "    '4': {'frequency_value': 40, 'frequency_location': 3},\n",
        "    '5': {'frequency_value': 0, 'frequency_location': 0},\n",
        "}\n",
        "\n",
        "print(\"Using manual frequency input for test cases:\")\n",
        "for case_id, freq_info in MANUAL_TEST_FREQUENCY.items():\n",
        "    if freq_info['frequency_value'] > 0:\n",
        "        print(f\"   Case {case_id}: {freq_info['frequency_value']}Hz at Section {freq_info['frequency_location']}\")\n",
        "    else:\n",
        "        print(f\"   Case {case_id}: No frequency defect\")\n",
        "\n",
        "# --- Configuration & Paths ---\n",
        "output_base_path = '/content/drive/MyDrive/IMES/imes/processed'\n",
        "section_data_path = os.path.join(output_base_path, 'section mapping')\n",
        "metadata_path = os.path.join(output_base_path, 'Combined_Metadata.csv')\n",
        "\n",
        "# --- Performance Metrics Function ---\n",
        "def calculate_comprehensive_metrics(y_true, y_pred, y_prob, dataset_name, defect_type):\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "    print(f\"\\n--- {dataset_name} - {defect_type} Performance Metrics ---\")\n",
        "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall:    {recall:.4f}\")\n",
        "    print(f\"F1-Score:  {f1:.4f}\")\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    print(f\"\\nConfusion Matrix:\")\n",
        "    print(cm)\n",
        "    print(f\"\\nClassification Report:\")\n",
        "    print(classification_report(y_true, y_pred, zero_division=0))\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1,\n",
        "        'confusion_matrix': cm\n",
        "    }\n",
        "\n",
        "# --- Visualization Function ---\n",
        "def plot_confusion_matrix(cm, title, labels=['No Defect', 'Defect']):\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.show()\n",
        "\n",
        "# --- Preprocessing Function ---\n",
        "def butter_lowpass_filter(data, cutoff=20, fs=100, order=5):\n",
        "    nyq = 0.5 * fs\n",
        "    normal_cutoff = cutoff / nyq\n",
        "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
        "    y = lfilter(b, a, data)\n",
        "    return y\n",
        "\n",
        "# --- 1. Load Metadata for Ground Truth ---\n",
        "print(\"\\nLoading combined metadata...\")\n",
        "try:\n",
        "    df_meta_full = pd.read_csv(metadata_path, dtype={'Case_ID': str})\n",
        "    ground_truth_cols = [\n",
        "        'Inclination Location', 'Damper Location',\n",
        "        'Frequency Value', 'Frequency Location', 'Dataset'\n",
        "    ]\n",
        "    df_meta_unique = df_meta_full.drop_duplicates(subset='Case_ID', keep='first').copy()\n",
        "    for col in ground_truth_cols:\n",
        "        if 'Location' in col or 'Value' in col:\n",
        "            df_meta_unique[col] = pd.to_numeric(df_meta_unique[col], errors='coerce').fillna(0).astype(int)\n",
        "    df_meta_unique.set_index('Case_ID', inplace=True)\n",
        "    meta_dict = df_meta_unique[ground_truth_cols].to_dict('index')\n",
        "    print(\"Metadata loaded and prepared.\")\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Error: Metadata file not found: {e.filename}\")\n",
        "    exit()\n",
        "\n",
        "# --- 2. Process Files ---\n",
        "print(\"\\nLoading files with dynamic section mapping...\")\n",
        "all_data = []\n",
        "processed_files = sorted([f for f in os.listdir(section_data_path) if f.startswith('sensor5_case') and f.endswith('.csv')])\n",
        "for filename in tqdm(processed_files, desc=\"Processing Files\"):\n",
        "    case_id_str = filename.replace('sensor5_case', '').replace('.csv', '')\n",
        "    file_path = os.path.join(section_data_path, filename)\n",
        "    try:\n",
        "        df_raw = pd.read_csv(file_path)\n",
        "        ground_truth = meta_dict.get(case_id_str, {})\n",
        "        if not ground_truth: continue\n",
        "        df_raw['inclination_label'] = (df_raw['section'] == ground_truth.get('Inclination Location', 0)).astype(int)\n",
        "        df_raw['damper_label'] = (df_raw['section'] == ground_truth.get('Damper Location', 0)).astype(int)\n",
        "        dataset_type = ground_truth.get('Dataset', '').lower()\n",
        "        if dataset_type == 'testing':\n",
        "            df_raw['inclination_label'] = 0\n",
        "            df_raw['damper_label'] = 0\n",
        "            manual_freq = MANUAL_TEST_FREQUENCY.get(case_id_str, {'frequency_value': 0, 'frequency_location': 0})\n",
        "            df_raw['frequency_value'] = manual_freq['frequency_value']\n",
        "            df_raw['frequency_label'] = (df_raw['section'] == manual_freq['frequency_location']).astype(int)\n",
        "        else:\n",
        "            df_raw['frequency_label'] = (df_raw['section'] == ground_truth.get('Frequency Location', 0)).astype(int)\n",
        "            df_raw['frequency_value'] = ground_truth.get('Frequency Value', 0)\n",
        "        df_raw['filtered_ax'] = butter_lowpass_filter(df_raw['ax'])\n",
        "        df_raw['filtered_ay'] = butter_lowpass_filter(df_raw['ay'])\n",
        "        df_raw['filtered_az'] = butter_lowpass_filter(df_raw['az'])\n",
        "        df_raw['dataset_type'] = dataset_type\n",
        "        all_data.append(df_raw)\n",
        "    except Exception as e:\n",
        "        continue\n",
        "\n",
        "df_full = pd.concat(all_data, ignore_index=True)\n",
        "df_full.dropna(inplace=True)\n",
        "print(f\"Full dataset prepared with {len(df_full)} data points using dynamic sections.\")\n",
        "\n",
        "# --- 3. Prepare Data for CNN ---\n",
        "print(\"\\nPreparing final data for the model...\")\n",
        "features_to_use = [\n",
        "    'Time',\n",
        "    'filtered_ax',\n",
        "    'filtered_ay',\n",
        "    'filtered_az',\n",
        "    'P25_acceleration',\n",
        "    'frequency_value',\n",
        "    'frequency_label'\n",
        "]\n",
        "labels_to_use = ['damper_label', 'inclination_label']\n",
        "X = df_full[features_to_use]\n",
        "y = df_full[labels_to_use]\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_imputed = imputer.fit_transform(X)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_imputed)\n",
        "X_reshaped = X_scaled.reshape((X_scaled.shape[0], X_scaled.shape[1], 1))\n",
        "y_damping_loc = to_categorical(y['damper_label'])\n",
        "y_inclination_loc = to_categorical(y['inclination_label'])\n",
        "train_mask = df_full['dataset_type'] == 'training'\n",
        "test_mask = df_full['dataset_type'] == 'testing'\n",
        "X_train, X_test = X_reshaped[train_mask], X_reshaped[test_mask]\n",
        "y_damping_train, y_damping_test = y_damping_loc[train_mask], y_damping_loc[test_mask]\n",
        "y_inclination_train, y_inclination_test = y_inclination_loc[train_mask], y_inclination_loc[test_mask]\n",
        "print(f\"Data prepared: {len(X_train)} training samples, {len(X_test)} testing samples.\")\n",
        "print(f\"Features used: {features_to_use}\")\n",
        "print(\"\\nTraining data distribution:\")\n",
        "train_damping_defects = np.sum(y_damping_train[:, 1])\n",
        "train_inclination_defects = np.sum(y_inclination_train[:, 1])\n",
        "print(f\"Damping defects: {train_damping_defects}/{len(y_damping_train)} ({train_damping_defects/len(y_damping_train)*100:.1f}%)\")\n",
        "print(f\"Inclination defects: {train_inclination_defects}/{len(y_inclination_train)} ({train_inclination_defects/len(y_inclination_train)*100:.1f}%)\")\n",
        "if len(X_train) == 0:\n",
        "    print(\"ERROR: No training data found!\")\n",
        "    exit()\n",
        "\n",
        "# --- 4. Build and Train Enhanced CNN ---\n",
        "print(\"\\nBuilding and training the enhanced CNN model...\")\n",
        "inputs = Input(shape=(X_train.shape[1], 1))\n",
        "conv_base = Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')(inputs)\n",
        "conv_base = MaxPooling1D(pool_size=2, padding='same')(conv_base)\n",
        "conv_base = Dropout(0.3)(conv_base)\n",
        "conv_base = Conv1D(filters=128, kernel_size=3, activation='relu', padding='same')(conv_base)\n",
        "conv_base = MaxPooling1D(pool_size=2, padding='same')(conv_base)\n",
        "conv_base = Dropout(0.3)(conv_base)\n",
        "conv_base = Conv1D(filters=256, kernel_size=3, activation='relu', padding='same')(conv_base)\n",
        "conv_base = MaxPooling1D(pool_size=2, padding='same')(conv_base)\n",
        "conv_base = Dropout(0.4)(conv_base)\n",
        "flat = Flatten()(conv_base)\n",
        "flat = Dense(128, activation='relu')(flat)\n",
        "flat = Dropout(0.5)(flat)\n",
        "output_damping = Dense(y_damping_train.shape[1], activation='softmax', name='damping_defect_output')(flat)\n",
        "output_inclination = Dense(y_inclination_train.shape[1], activation='softmax', name='inclination_defect_output')(flat)\n",
        "model = Model(inputs=inputs, outputs=[output_damping, output_inclination])\n",
        "optimizer = Adam(learning_rate=0.0005)\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss={'damping_defect_output': 'binary_crossentropy', 'inclination_defect_output': 'binary_crossentropy'},\n",
        "    metrics={'damping_defect_output': 'accuracy', 'inclination_defect_output': 'accuracy'}\n",
        ")\n",
        "model.summary()\n",
        "history = model.fit(\n",
        "    X_train,\n",
        "    {'damping_defect_output': y_damping_train, 'inclination_defect_output': y_inclination_train},\n",
        "    epochs=15,\n",
        "    batch_size=64,\n",
        "    validation_split=0.2,\n",
        "    verbose=1\n",
        ")\n",
        "print(\"Model training complete.\")\n",
        "\n",
        "# --- 5. Performance Evaluation ---\n",
        "print(\"\\nComprehensive Performance Evaluation...\")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINING PERFORMANCE METRICS\")\n",
        "print(\"=\"*60)\n",
        "y_train_pred = model.predict(X_train, verbose=0)\n",
        "y_train_damping_decoded = np.argmax(y_train_pred[0], axis=1)\n",
        "y_train_inclination_decoded = np.argmax(y_train_pred[1], axis=1)\n",
        "y_train_damping_true = np.argmax(y_damping_train, axis=1)\n",
        "y_train_inclination_true = np.argmax(y_inclination_train, axis=1)\n",
        "train_damping_metrics = calculate_comprehensive_metrics(\n",
        "    y_train_damping_true, y_train_damping_decoded, y_train_pred[0][:, 1], \"TRAINING\", \"DAMPING DEFECT\"\n",
        ")\n",
        "train_inclination_metrics = calculate_comprehensive_metrics(\n",
        "    y_train_inclination_true, y_train_inclination_decoded, y_train_pred[1][:, 1], \"TRAINING\", \"INCLINATION DEFECT\"\n",
        ")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TESTING PERFORMANCE METRICS\")\n",
        "print(\"=\"*60)\n",
        "y_test_pred = model.predict(X_test, verbose=0)\n",
        "y_test_damping_decoded = np.argmax(y_test_pred[0], axis=1)\n",
        "y_test_inclination_decoded = np.argmax(y_test_pred[1], axis=1)\n",
        "y_test_damping_true = np.argmax(y_damping_test, axis=1)\n",
        "y_test_inclination_true = np.argmax(y_inclination_test, axis=1)\n",
        "test_damping_metrics = calculate_comprehensive_metrics(\n",
        "    y_test_damping_true, y_test_damping_decoded, y_test_pred[0][:, 1], \"TESTING\", \"DAMPING DEFECT\"\n",
        ")\n",
        "test_inclination_metrics = calculate_comprehensive_metrics(\n",
        "    y_test_inclination_true, y_test_inclination_decoded, y_test_pred[1][:, 1], \"TESTING\", \"INCLINATION DEFECT\"\n",
        ")\n",
        "print(\"\\nPrediction probability analysis:\")\n",
        "print(f\"Damping probabilities - Min: {y_test_pred[0][:, 1].min():.3f}, Max: {y_test_pred[0][:, 1].max():.3f}, Mean: {y_test_pred[0][:, 1].mean():.3f}\")\n",
        "print(f\"Inclination probabilities - Min: {y_test_pred[1][:, 1].min():.3f}, Max: {y_test_pred[1][:, 1].max():.3f}, Mean: {y_test_pred[1][:, 1].mean():.3f}\")\n",
        "\n",
        "# --- 7. Visualization of Results ---\n",
        "plot_confusion_matrix(train_damping_metrics['confusion_matrix'],\n",
        "                     'Training - Damping Defect Confusion Matrix')\n",
        "plot_confusion_matrix(train_inclination_metrics['confusion_matrix'],\n",
        "                     'Training - Inclination Defect Confusion Matrix')\n",
        "plot_confusion_matrix(test_damping_metrics['confusion_matrix'],\n",
        "                     'Testing - Damping Defect Confusion Matrix')\n",
        "plot_confusion_matrix(test_inclination_metrics['confusion_matrix'],\n",
        "                     'Testing - Inclination Defect Confusion Matrix')\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(history.history['damping_defect_output_accuracy'], label='Training Damping Accuracy')\n",
        "plt.plot(history.history['val_damping_defect_output_accuracy'], label='Validation Damping Accuracy')\n",
        "plt.title('Damping Defect Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.plot(history.history['inclination_defect_output_accuracy'], label='Training Inclination Accuracy')\n",
        "plt.plot(history.history['val_inclination_defect_output_accuracy'], label='Validation Inclination Accuracy')\n",
        "plt.title('Inclination Defect Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- 8. Summary Performance Table ---\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PERFORMANCE SUMMARY TABLE\")\n",
        "print(\"=\"*80)\n",
        "summary_data = {\n",
        "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score'],\n",
        "    'Train_Damping': [\n",
        "        train_damping_metrics['accuracy'],\n",
        "        train_damping_metrics['precision'],\n",
        "        train_damping_metrics['recall'],\n",
        "        train_damping_metrics['f1_score']\n",
        "    ],\n",
        "    'Test_Damping': [\n",
        "        test_damping_metrics['accuracy'],\n",
        "        test_damping_metrics['precision'],\n",
        "        test_damping_metrics['recall'],\n",
        "        test_damping_metrics['f1_score']\n",
        "    ],\n",
        "    'Train_Inclination': [\n",
        "        train_inclination_metrics['accuracy'],\n",
        "        train_inclination_metrics['precision'],\n",
        "        train_inclination_metrics['recall'],\n",
        "        train_inclination_metrics['f1_score']\n",
        "    ],\n",
        "    'Test_Inclination': [\n",
        "        test_inclination_metrics['accuracy'],\n",
        "        test_inclination_metrics['precision'],\n",
        "        test_inclination_metrics['recall'],\n",
        "        test_inclination_metrics['f1_score']\n",
        "    ]\n",
        "}\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "print(summary_df.round(4).to_string(index=False))\n",
        "\n",
        "# --- 9. Generate Case-Level Predictions ---\n",
        "print(\"\\nGenerating case-level predictions...\")\n",
        "test_case_ids = df_full[test_mask]['case_id'].values\n",
        "df_predictions = pd.DataFrame({\n",
        "    'Case_ID': test_case_ids,\n",
        "    'Damping_Prediction': y_test_damping_decoded,\n",
        "    'Inclination_Prediction': y_test_inclination_decoded,\n",
        "    'Damping_Probability': y_test_pred[0][:, 1],\n",
        "    'Inclination_Probability': y_test_pred[1][:, 1]\n",
        "})\n",
        "\n",
        "def conservative_case_prediction(group, threshold=0.15):\n",
        "    return 1 if (group.sum() / len(group)) > threshold else 0\n",
        "\n",
        "final_predictions = df_predictions.groupby('Case_ID').agg({\n",
        "    'Damping_Prediction': lambda x: conservative_case_prediction(x),\n",
        "    'Inclination_Prediction': lambda x: conservative_case_prediction(x),\n",
        "    'Damping_Probability': 'mean',\n",
        "    'Inclination_Probability': 'mean'\n",
        "}).reset_index()\n",
        "\n",
        "final_predictions['Damping_Defect'] = final_predictions['Damping_Prediction'].map({0: 'No', 1: 'Yes'})\n",
        "final_predictions['Inclination_Defect'] = final_predictions['Inclination_Prediction'].map({0: 'No', 1: 'Yes'})\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"         FINAL DEFECT PREDICTIONS FOR TEST CASES\")\n",
        "print(\"=\"*80)\n",
        "results_table = final_predictions[['Case_ID', 'Inclination_Defect', 'Damping_Defect', 'Inclination_Probability', 'Damping_Probability']].copy()\n",
        "results_table.columns = ['Case_ID', 'Inclination Defect', 'Damper Defect', 'Inclination Prob', 'Damper Prob']\n",
        "results_table['Inclination Prob'] = results_table['Inclination Prob'].round(3)\n",
        "results_table['Damper Prob'] = results_table['Damper Prob'].round(3)\n",
        "print(results_table.to_string(index=False))\n",
        "\n",
        "# --- 10. Save Results and Metrics ---\n",
        "print(\"\\nSaving results and metrics...\")\n",
        "\n",
        "output_file = os.path.join(output_base_path, 'test_predictions_dynamic_sections.csv')\n",
        "results_table.to_csv(output_file, index=False)\n",
        "metrics_file = os.path.join(output_base_path, 'performance_metrics_dynamic_sections.csv')\n",
        "summary_df.to_csv(metrics_file, index=False)\n",
        "model_file = os.path.join(output_base_path, 'cnn_defect_model_dynamic_sections.h5')\n",
        "model.save(model_file)\n",
        "\n",
        "print(f\"Predictions saved to: {output_file}\")\n",
        "print(f\"Metrics saved to: {metrics_file}\")\n",
        "print(f\"Model saved to: {model_file}\")\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Prediction process finished.\")\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SINGLE OUTPUT MODELS FOR DAMPER AND INCLINATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6lZ2ZFB746t"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from scipy.signal import butter, lfilter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.metrics import f1_score, confusion_matrix, classification_report, accuracy_score, precision_score, recall_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"DAMPER DEFECT DETECTION - ADVANCED DENSE NN\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "output_base_path = '/content/drive/MyDrive/IMES/imes/processed'\n",
        "section_data_path = os.path.join(output_base_path, 'section mapping')\n",
        "metadata_path = os.path.join(output_base_path, 'Combined_Metadata.csv')\n",
        "\n",
        "def calculate_metrics(y_true, y_pred, y_prob, dataset_name):\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "    precision_per_class = precision_score(y_true, y_pred, average=None, zero_division=0)\n",
        "    recall_per_class = recall_score(y_true, y_pred, average=None, zero_division=0)\n",
        "    f1_per_class = f1_score(y_true, y_pred, average=None, zero_division=0)\n",
        "\n",
        "    print(f\"\\n--- {dataset_name} - DAMPER DEFECT Performance Metrics ---\")\n",
        "    print(f\"Overall Accuracy:  {accuracy:.4f}\")\n",
        "    print(f\"Overall Precision: {precision:.4f}\")\n",
        "    print(f\"Overall Recall:    {recall:.4f}\")\n",
        "    print(f\"Overall F1-Score:  {f1:.4f}\")\n",
        "    print(f\"\\nPer-Class Metrics:\")\n",
        "    print(f\"No Damper Defect   - Precision: {precision_per_class[0]:.4f}, Recall: {recall_per_class[0]:.4f}, F1: {f1_per_class[0]:.4f}\")\n",
        "    if len(precision_per_class) > 1:\n",
        "        print(f\"Damper Defect      - Precision: {precision_per_class[1]:.4f}, Recall: {recall_per_class[1]:.4f}, F1: {f1_per_class[1]:.4f}\")\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1,\n",
        "        'confusion_matrix': cm,\n",
        "        'precision_per_class': precision_per_class,\n",
        "        'recall_per_class': recall_per_class,\n",
        "        'f1_per_class': f1_per_class\n",
        "    }\n",
        "\n",
        "def focal_loss(gamma=2., alpha=0.25):\n",
        "    def focal_loss_fixed(y_true, y_pred):\n",
        "        epsilon = tf.keras.backend.epsilon()\n",
        "        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
        "        y_true_binary = tf.cast(tf.argmax(y_true, axis=1), tf.float32)\n",
        "        y_pred_binary = y_pred[:, 1]\n",
        "        p_t = tf.where(tf.equal(y_true_binary, 1), y_pred_binary, 1 - y_pred_binary)\n",
        "        alpha_factor = tf.ones_like(y_true_binary) * alpha\n",
        "        alpha_t = tf.where(tf.equal(y_true_binary, 1), alpha_factor, 1 - alpha_factor)\n",
        "        cross_entropy = -tf.math.log(p_t)\n",
        "        weight = alpha_t * tf.pow((1 - p_t), gamma)\n",
        "        loss = weight * cross_entropy\n",
        "        return tf.reduce_mean(loss)\n",
        "    return focal_loss_fixed\n",
        "\n",
        "def butter_lowpass_filter(data, cutoff=20, fs=100, order=5):\n",
        "    nyq = 0.5 * fs\n",
        "    normal_cutoff = cutoff / nyq\n",
        "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
        "    y = lfilter(b, a, data)\n",
        "    return y\n",
        "\n",
        "def create_enhanced_features(df):\n",
        "    window_size = 10\n",
        "    df['ax_rolling_mean'] = df.groupby('case_id')['filtered_ax'].rolling(window=window_size, min_periods=1).mean().reset_index(level=0, drop=True)\n",
        "    df['ax_rolling_std'] = df.groupby('case_id')['filtered_ax'].rolling(window=window_size, min_periods=1).std().reset_index(level=0, drop=True)\n",
        "    df['ay_rolling_mean'] = df.groupby('case_id')['filtered_ay'].rolling(window=window_size, min_periods=1).mean().reset_index(level=0, drop=True)\n",
        "    df['az_rolling_std'] = df.groupby('case_id')['filtered_az'].rolling(window=window_size, min_periods=1).std().reset_index(level=0, drop=True)\n",
        "    df.fillna(method='bfill', inplace=True)\n",
        "    df.fillna(0, inplace=True)\n",
        "    df['acceleration_magnitude'] = np.sqrt(df['filtered_ax']**2 + df['filtered_ay']**2 + df['filtered_az']**2)\n",
        "    df['acceleration_change'] = df.groupby('case_id')['acceleration_magnitude'].diff().abs()\n",
        "    df['acceleration_change'].fillna(0, inplace=True)\n",
        "    return df\n",
        "\n",
        "print(\"\\nLoading combined metadata...\")\n",
        "try:\n",
        "    df_meta_full = pd.read_csv(metadata_path, dtype={'Case_ID': str})\n",
        "    ground_truth_cols = [\n",
        "        'Inclination Location', 'Damper Location',\n",
        "        'Frequency Value', 'Frequency Location', 'Dataset'\n",
        "    ]\n",
        "    df_meta_unique = df_meta_full.drop_duplicates(subset='Case_ID', keep='first').copy()\n",
        "    for col in ground_truth_cols:\n",
        "        if 'Location' in col or 'Value' in col:\n",
        "            df_meta_unique[col] = pd.to_numeric(df_meta_unique[col], errors='coerce').fillna(0).astype(int)\n",
        "    df_meta_unique.set_index('Case_ID', inplace=True)\n",
        "    meta_dict = df_meta_unique[ground_truth_cols].to_dict('index')\n",
        "    print(\"Metadata loaded and prepared.\")\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Error: Metadata file not found: {e.filename}\")\n",
        "    exit()\n",
        "\n",
        "print(\"\\nLoading files with enhanced feature engineering...\")\n",
        "all_data = []\n",
        "processed_files = sorted([f for f in os.listdir(section_data_path) if f.startswith('sensor5_case') and f.endswith('.csv')])\n",
        "for filename in tqdm(processed_files, desc=\"Processing Files\"):\n",
        "    case_id_str = filename.replace('sensor5_case', '').replace('.csv', '')\n",
        "    file_path = os.path.join(section_data_path, filename)\n",
        "    try:\n",
        "        df_raw = pd.read_csv(file_path)\n",
        "        ground_truth = meta_dict.get(case_id_str, {})\n",
        "        if not ground_truth: continue\n",
        "        df_raw['damper_label'] = (df_raw['section'] == ground_truth.get('Damper Location', 0)).astype(int)\n",
        "        df_raw['filtered_ax'] = butter_lowpass_filter(df_raw['ax'])\n",
        "        df_raw['filtered_ay'] = butter_lowpass_filter(df_raw['ay'])\n",
        "        df_raw['filtered_az'] = butter_lowpass_filter(df_raw['az'])\n",
        "        dataset_type = ground_truth.get('Dataset', '').lower()\n",
        "        df_raw['dataset_type'] = dataset_type\n",
        "        df_raw['case_id'] = case_id_str\n",
        "        all_data.append(df_raw)\n",
        "    except Exception:\n",
        "        continue\n",
        "\n",
        "df_full = pd.concat(all_data, ignore_index=True)\n",
        "df_full.dropna(inplace=True)\n",
        "df_full = create_enhanced_features(df_full)\n",
        "\n",
        "section_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "section_encoded = section_encoder.fit_transform(df_full[['section']])\n",
        "section_columns = [f'section_{i}' for i in range(section_encoded.shape[1])]\n",
        "base_features = [\n",
        "    'Time',\n",
        "    'filtered_ax', 'filtered_ay', 'filtered_az',\n",
        "    'P25_acceleration',\n",
        "    'ax_rolling_mean', 'ax_rolling_std',\n",
        "    'ay_rolling_mean', 'az_rolling_std',\n",
        "    'acceleration_magnitude', 'acceleration_change'\n",
        "]\n",
        "X_base = df_full[base_features]\n",
        "X_sections = pd.DataFrame(section_encoded, columns=section_columns, index=df_full.index)\n",
        "X = pd.concat([X_base, X_sections], axis=1)\n",
        "y = df_full['damper_label']\n",
        "\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_imputed = imputer.fit_transform(X)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_imputed)\n",
        "y_damper = to_categorical(y)\n",
        "\n",
        "train_mask = df_full['dataset_type'] == 'training'\n",
        "test_mask = df_full['dataset_type'] == 'testing'\n",
        "X_train_full, X_test = X_scaled[train_mask], X_scaled[test_mask]\n",
        "y_damper_train_full, y_damper_test = y_damper[train_mask], y_damper[test_mask]\n",
        "\n",
        "X_train, X_val, y_damper_train, y_damper_val = train_test_split(\n",
        "    X_train_full, y_damper_train_full,\n",
        "    test_size=0.2, random_state=42, stratify=y_damper_train_full[:, 1]\n",
        ")\n",
        "\n",
        "print(f\"Train/Val/Test splits: {len(X_train)} / {len(X_val)} / {len(X_test)}\")\n",
        "\n",
        "print(\"\\nApplying SMOTE for damper defect class balancing...\")\n",
        "smote = SMOTE(random_state=42, k_neighbors=3)\n",
        "X_train_balanced, y_damper_balanced = smote.fit_resample(X_train, y_damper_train.argmax(axis=1))\n",
        "y_damper_balanced = to_categorical(y_damper_balanced)\n",
        "print(f\"SMOTE applied: Balanced damper defects: {y_damper_balanced[:, 1].sum()}/{len(y_damper_balanced)} ({y_damper_balanced[:, 1].sum()/len(y_damper_balanced)*100:.1f}%)\")\n",
        "\n",
        "inputs = Input(shape=(X_train_balanced.shape[1],))\n",
        "x = Dense(512, activation='relu')(inputs)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.3)(x)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.3)(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.4)(x)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "x = Dropout(0.4)(x)\n",
        "output_damper = Dense(2, activation='softmax', name='damper_defect_output')(x)\n",
        "model = Model(inputs=inputs, outputs=output_damper)\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=focal_loss(gamma=2.0, alpha=0.25),\n",
        "    metrics=['accuracy', 'precision', 'recall']\n",
        ")\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=7, min_lr=1e-6, verbose=1)\n",
        "]\n",
        "print(\"\\nTraining damper defect detection model...\")\n",
        "history = model.fit(\n",
        "    X_train_balanced, y_damper_balanced,\n",
        "    validation_data=(X_val, y_damper_val),\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "print(\"Damper defect model training complete.\")\n",
        "\n",
        "print(\"\\nDamper Defect Detection Performance...\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINING PERFORMANCE METRICS\")\n",
        "print(\"=\"*60)\n",
        "y_train_pred = model.predict(X_train_balanced, verbose=0)\n",
        "y_train_decoded = np.argmax(y_train_pred, axis=1)\n",
        "y_train_true = np.argmax(y_damper_balanced, axis=1)\n",
        "train_metrics = calculate_metrics(y_train_true, y_train_decoded, y_train_pred[:, 1], \"TRAINING\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"VALIDATION PERFORMANCE METRICS\")\n",
        "print(\"=\"*60)\n",
        "y_val_pred = model.predict(X_val, verbose=0)\n",
        "y_val_decoded = np.argmax(y_val_pred, axis=1)\n",
        "y_val_true = np.argmax(y_damper_val, axis=1)\n",
        "val_metrics = calculate_metrics(y_val_true, y_val_decoded, y_val_pred[:, 1], \"VALIDATION\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TESTING PERFORMANCE METRICS\")\n",
        "print(\"=\"*60)\n",
        "y_test_pred = model.predict(X_test, verbose=0)\n",
        "y_test_decoded = np.argmax(y_test_pred, axis=1)\n",
        "y_test_true = np.argmax(y_damper_test, axis=1)\n",
        "test_metrics = calculate_metrics(y_test_true, y_test_decoded, y_test_pred[:, 1], \"TESTING\")\n",
        "\n",
        "print(\"\\nGenerating damper defect predictions...\")\n",
        "test_data = df_full[df_full['dataset_type'] == 'testing'].copy()\n",
        "test_data = test_data.reset_index(drop=True)\n",
        "test_data['Damper_Prediction'] = y_test_decoded\n",
        "test_data['Damper_Probability'] = y_test_pred[:, 1]\n",
        "output_columns = [\n",
        "    'case_id', 'Time', 'section',\n",
        "    'ax', 'ay', 'az',\n",
        "    'filtered_ax', 'filtered_ay', 'filtered_az',\n",
        "    'Damper_Prediction', 'Damper_Probability'\n",
        "]\n",
        "damper_timestamps_df = test_data[output_columns].copy()\n",
        "damper_timestamps_df = damper_timestamps_df.sort_values(['case_id', 'Time'])\n",
        "\n",
        "df_predictions = pd.DataFrame({\n",
        "    'Case_ID': test_data['case_id'].values,\n",
        "    'Damper_Prediction': y_test_decoded,\n",
        "    'Damper_Probability': y_test_pred[:, 1]\n",
        "})\n",
        "def damper_case_prediction(group, threshold=0.05):\n",
        "    return 1 if (group.sum() / len(group)) > threshold else 0\n",
        "final_predictions = df_predictions.groupby('Case_ID').agg({\n",
        "    'Damper_Prediction': lambda x: damper_case_prediction(x, threshold=0.05),\n",
        "    'Damper_Probability': 'mean'\n",
        "}).reset_index()\n",
        "final_predictions['Damper_Defect'] = final_predictions['Damper_Prediction'].map({0: 'No', 1: 'Yes'})\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FINAL DAMPER DEFECT PREDICTIONS FOR TEST CASES\")\n",
        "print(\"=\"*80)\n",
        "results_table = final_predictions[['Case_ID', 'Damper_Defect', 'Damper_Probability']].copy()\n",
        "results_table['Damper_Probability'] = results_table['Damper_Probability'].round(3)\n",
        "print(results_table.to_string(index=False))\n",
        "\n",
        "print(\"\\nSaving damper defect detection results...\")\n",
        "timestamp_output_path = os.path.join(output_base_path, 'damper_defect_timestamps.csv')\n",
        "damper_timestamps_df.to_csv(timestamp_output_path, index=False)\n",
        "case_output_path = os.path.join(output_base_path, 'damper_defect_cases.csv')\n",
        "results_table.to_csv(case_output_path, index=False)\n",
        "model_file = os.path.join(output_base_path, 'damper_defect_model.keras')\n",
        "model.save(model_file)\n",
        "print(f\"Timestamp predictions: {timestamp_output_path}\")\n",
        "print(f\"Case predictions: {case_output_path}\")\n",
        "print(f\"Model saved: {model_file}\")\n",
        "\n",
        "print(f\"\\nSummary Statistics:\")\n",
        "print(f\"Total testing timestamps: {len(damper_timestamps_df)}\")\n",
        "print(f\"Timestamps with damper defects: {damper_timestamps_df['Damper_Prediction'].sum()}\")\n",
        "case_summary = damper_timestamps_df.groupby('case_id').agg({\n",
        "    'Damper_Prediction': 'sum',\n",
        "    'Time': 'count'\n",
        "}).rename(columns={'Time': 'Total_Timestamps'})\n",
        "print(f\"\\nDamper Defects by Case (Timestamp Level):\")\n",
        "print(case_summary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loqkCwaH8Vpt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from scipy.signal import butter, lfilter\n",
        "from scipy.integrate import cumulative_trapezoid as cumtrapz\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.metrics import f1_score, confusion_matrix, classification_report, accuracy_score, precision_score, recall_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"INCLINATION DEFECT DETECTION - LINEAR ACCELERATION APPROACH\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Configuration & Paths\n",
        "output_base_path = '/content/drive/MyDrive/IMES/imes/processed'\n",
        "section_data_path = os.path.join(output_base_path, 'section mapping')\n",
        "metadata_path = os.path.join(output_base_path, 'Combined_Metadata.csv')\n",
        "\n",
        "def create_inclination_features(df):\n",
        "    df['acceleration_magnitude'] = np.sqrt(df['ax']**2 + df['ay']**2 + df['az']**2)\n",
        "    df['horizontal_acceleration'] = np.sqrt(df['ax']**2 + df['ay']**2)\n",
        "    df['vertical_acceleration'] = df['az']\n",
        "    window = 10\n",
        "    for axis in ['ax', 'ay', 'az']:\n",
        "        df[f'{axis}_rolling_mean'] = df[axis].rolling(window=window, min_periods=1).mean()\n",
        "        df[f'{axis}_rolling_std'] = df[axis].rolling(window=window, min_periods=1).std()\n",
        "        df[f'{axis}_rolling_max'] = df[axis].rolling(window=window, min_periods=1).max()\n",
        "        df[f'{axis}_rolling_min'] = df[axis].rolling(window=window, min_periods=1).min()\n",
        "    df['ax_rate'] = df['ax'].diff()\n",
        "    df['ay_rate'] = df['ay'].diff()\n",
        "    df['az_rate'] = df['az'].diff()\n",
        "    df['acceleration_change'] = np.sqrt(df['ax_rate']**2 + df['ay_rate']**2 + df['az_rate']**2)\n",
        "    df['vertical_acceleration_change'] = np.abs(df['az_rate'])\n",
        "    df['velocity_x'] = cumtrapz(df['ax'], df['Time'], initial=0)\n",
        "    df['velocity_y'] = cumtrapz(df['ay'], df['Time'], initial=0)\n",
        "    df['velocity_z'] = cumtrapz(df['az'], df['Time'], initial=0)\n",
        "    df['velocity_magnitude'] = np.sqrt(df['velocity_x']**2 + df['velocity_y']**2 + df['velocity_z']**2)\n",
        "    df['inclination_indicator'] = np.abs(df['az_rolling_mean'])\n",
        "    df['motion_intensity'] = df['acceleration_magnitude'].rolling(window=20, min_periods=1).std()\n",
        "    df['vertical_motion_intensity'] = df['vertical_acceleration'].rolling(window=20, min_periods=1).std()\n",
        "    df['acceleration_tilt_ratio'] = df['vertical_acceleration'] / (df['horizontal_acceleration'] + 1e-6)\n",
        "    df['velocity_tilt_ratio'] = df['velocity_z'] / (np.sqrt(df['velocity_x']**2 + df['velocity_y']**2) + 1e-6)\n",
        "    df['cumulative_vertical_acceleration'] = df['vertical_acceleration'].cumsum()\n",
        "    df['cumulative_acceleration_change'] = df['acceleration_change'].cumsum()\n",
        "    df.fillna(method='bfill', inplace=True)\n",
        "    df.fillna(0, inplace=True)\n",
        "    return df\n",
        "\n",
        "def focal_loss(gamma=2., alpha=0.25):\n",
        "    def focal_loss_fixed(y_true, y_pred):\n",
        "        epsilon = tf.keras.backend.epsilon()\n",
        "        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
        "        y_true_binary = tf.cast(tf.argmax(y_true, axis=1), tf.float32)\n",
        "        y_pred_binary = y_pred[:, 1]\n",
        "        p_t = tf.where(tf.equal(y_true_binary, 1), y_pred_binary, 1 - y_pred_binary)\n",
        "        alpha_factor = tf.ones_like(y_true_binary) * alpha\n",
        "        alpha_t = tf.where(tf.equal(y_true_binary, 1), alpha_factor, 1 - alpha_factor)\n",
        "        cross_entropy = -tf.math.log(p_t)\n",
        "        weight = alpha_t * tf.pow((1 - p_t), gamma)\n",
        "        loss = weight * cross_entropy\n",
        "        return tf.reduce_mean(loss)\n",
        "    return focal_loss_fixed\n",
        "\n",
        "def calculate_inclination_metrics(y_true, y_pred, y_prob, dataset_name):\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "    precision_per_class = precision_score(y_true, y_pred, average=None, zero_division=0)\n",
        "    recall_per_class = recall_score(y_true, y_pred, average=None, zero_division=0)\n",
        "    f1_per_class = f1_score(y_true, y_pred, average=None, zero_division=0)\n",
        "    print(f\"\\n--- {dataset_name} - INCLINATION DEFECT Performance Metrics ---\")\n",
        "    print(f\"Overall Accuracy:  {accuracy:.4f}\")\n",
        "    print(f\"Overall Precision: {precision:.4f}\")\n",
        "    print(f\"Overall Recall:    {recall:.4f}\")\n",
        "    print(f\"Overall F1-Score:  {f1:.4f}\")\n",
        "    print(f\"\\nPer-Class Metrics:\")\n",
        "    print(f\"No Inclination Defect  - Precision: {precision_per_class[0]:.4f}, Recall: {recall_per_class[0]:.4f}, F1: {f1_per_class[0]:.4f}\")\n",
        "    if len(precision_per_class) > 1:\n",
        "        print(f\"Inclination Defect     - Precision: {precision_per_class[1]:.4f}, Recall: {recall_per_class[1]:.4f}, F1: {f1_per_class[1]:.4f}\")\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    print(f\"\\nConfusion Matrix:\")\n",
        "    print(cm)\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1,\n",
        "        'confusion_matrix': cm,\n",
        "        'precision_per_class': precision_per_class,\n",
        "        'recall_per_class': recall_per_class,\n",
        "        'f1_per_class': f1_per_class\n",
        "    }\n",
        "\n",
        "try:\n",
        "    df_meta_full = pd.read_csv(metadata_path, dtype={'Case_ID': str})\n",
        "    df_meta_unique = df_meta_full.drop_duplicates(subset='Case_ID', keep='first').copy()\n",
        "    for col in ['Inclination Location']:\n",
        "        df_meta_unique[col] = pd.to_numeric(df_meta_unique[col], errors='coerce').fillna(0).astype(int)\n",
        "    df_meta_unique.set_index('Case_ID', inplace=True)\n",
        "    meta_dict = df_meta_unique.to_dict('index')\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Error: Metadata file not found: {e.filename}\")\n",
        "    exit()\n",
        "\n",
        "all_data = []\n",
        "processed_files = sorted([f for f in os.listdir(section_data_path) if f.startswith('sensor5_case') and f.endswith('.csv')])\n",
        "for filename in tqdm(processed_files, desc=\"Processing Files\"):\n",
        "    case_id_str = filename.replace('sensor5_case', '').replace('.csv', '')\n",
        "    file_path = os.path.join(section_data_path, filename)\n",
        "    try:\n",
        "        df_raw = pd.read_csv(file_path)\n",
        "        ground_truth = meta_dict.get(case_id_str, {})\n",
        "        if not ground_truth: continue\n",
        "        df_raw['inclination_label'] = (df_raw['section'] == ground_truth.get('Inclination Location', 0)).astype(int)\n",
        "        df_raw = create_inclination_features(df_raw)\n",
        "        dataset_type = ground_truth.get('Dataset', '').lower()\n",
        "        df_raw['dataset_type'] = dataset_type\n",
        "        df_raw['case_id'] = case_id_str\n",
        "        all_data.append(df_raw)\n",
        "    except Exception:\n",
        "        continue\n",
        "\n",
        "df_full = pd.concat(all_data, ignore_index=True)\n",
        "df_full.dropna(inplace=True)\n",
        "\n",
        "section_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "section_encoded = section_encoder.fit_transform(df_full[['section']])\n",
        "section_columns = [f'section_{i}' for i in range(section_encoded.shape[1])]\n",
        "\n",
        "inclination_features = [\n",
        "    'Time', 'ax', 'ay', 'az', 'acceleration_magnitude', 'horizontal_acceleration', 'vertical_acceleration',\n",
        "    'ax_rolling_mean', 'ay_rolling_mean', 'az_rolling_mean',\n",
        "    'ax_rolling_std', 'ay_rolling_std', 'az_rolling_std',\n",
        "    'ax_rate', 'ay_rate', 'az_rate',\n",
        "    'acceleration_change', 'vertical_acceleration_change',\n",
        "    'velocity_x', 'velocity_y', 'velocity_z', 'velocity_magnitude',\n",
        "    'inclination_indicator', 'motion_intensity', 'vertical_motion_intensity',\n",
        "    'acceleration_tilt_ratio', 'velocity_tilt_ratio',\n",
        "    'cumulative_vertical_acceleration', 'cumulative_acceleration_change'\n",
        "]\n",
        "X_base = df_full[inclination_features]\n",
        "X_sections = pd.DataFrame(section_encoded, columns=section_columns, index=df_full.index)\n",
        "X = pd.concat([X_base, X_sections], axis=1)\n",
        "y = df_full['inclination_label']\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_imputed = imputer.fit_transform(X)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_imputed)\n",
        "y_inclination = to_categorical(y)\n",
        "train_mask = df_full['dataset_type'] == 'training'\n",
        "test_mask = df_full['dataset_type'] == 'testing'\n",
        "X_train_full, X_test = X_scaled[train_mask], X_scaled[test_mask]\n",
        "y_inclination_train_full, y_inclination_test = y_inclination[train_mask], y_inclination[test_mask]\n",
        "X_train, X_val, y_inclination_train, y_inclination_val = train_test_split(\n",
        "    X_train_full, y_inclination_train_full, test_size=0.2, random_state=42, stratify=y_inclination_train_full[:, 1]\n",
        ")\n",
        "\n",
        "smote = SMOTE(random_state=42, k_neighbors=3)\n",
        "X_train_balanced, y_inclination_balanced = smote.fit_resample(X_train, y_inclination_train.argmax(axis=1))\n",
        "y_inclination_balanced = to_categorical(y_inclination_balanced)\n",
        "\n",
        "inputs = Input(shape=(X_train_balanced.shape[1],))\n",
        "x = Dense(512, activation='relu')(inputs)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.3)(x)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.3)(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.4)(x)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "x = Dropout(0.4)(x)\n",
        "output_inclination = Dense(2, activation='softmax', name='inclination_defect_output')(x)\n",
        "model = Model(inputs=inputs, outputs=output_inclination)\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=focal_loss(gamma=2.0, alpha=0.25),\n",
        "    metrics=['accuracy', 'precision', 'recall']\n",
        ")\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=7, min_lr=1e-6, verbose=1)\n",
        "]\n",
        "history = model.fit(\n",
        "    X_train_balanced, y_inclination_balanced,\n",
        "    validation_data=(X_val, y_inclination_val),\n",
        "    epochs=5,\n",
        "    batch_size=32,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "y_train_pred = model.predict(X_train_balanced, verbose=0)\n",
        "y_train_decoded = np.argmax(y_train_pred, axis=1)\n",
        "y_train_true = np.argmax(y_inclination_balanced, axis=1)\n",
        "train_metrics = calculate_inclination_metrics(\n",
        "    y_train_true, y_train_decoded, y_train_pred[:, 1], \"TRAINING\"\n",
        ")\n",
        "y_val_pred = model.predict(X_val, verbose=0)\n",
        "y_val_decoded = np.argmax(y_val_pred, axis=1)\n",
        "y_val_true = np.argmax(y_inclination_val, axis=1)\n",
        "val_metrics = calculate_inclination_metrics(\n",
        "    y_val_true, y_val_decoded, y_val_pred[:, 1], \"VALIDATION\"\n",
        ")\n",
        "y_test_pred = model.predict(X_test, verbose=0)\n",
        "y_test_decoded = np.argmax(y_test_pred, axis=1)\n",
        "y_test_true = np.argmax(y_inclination_test, axis=1)\n",
        "test_metrics = calculate_inclination_metrics(\n",
        "    y_test_true, y_test_decoded, y_test_pred[:, 1], \"TESTING\"\n",
        ")\n",
        "test_data = df_full[df_full['dataset_type'] == 'testing'].copy()\n",
        "test_data = test_data.reset_index(drop=True)\n",
        "test_data['Inclination_Prediction'] = y_test_decoded\n",
        "test_data['Inclination_Probability'] = y_test_pred[:, 1]\n",
        "output_columns = [\n",
        "    'case_id', 'Time', 'section',\n",
        "    'ax', 'ay', 'az',\n",
        "    'vertical_acceleration', 'inclination_indicator',\n",
        "    'Inclination_Prediction', 'Inclination_Probability'\n",
        "]\n",
        "inclination_timestamps_df = test_data[output_columns].copy()\n",
        "inclination_timestamps_df = inclination_timestamps_df.sort_values(['case_id', 'Time'])\n",
        "df_predictions = pd.DataFrame({\n",
        "    'Case_ID': test_data['case_id'].values,\n",
        "    'Inclination_Prediction': y_test_decoded,\n",
        "    'Inclination_Probability': y_test_pred[:, 1]\n",
        "})\n",
        "def inclination_case_prediction(group, threshold=0.05):\n",
        "    return 1 if (group.sum() / len(group)) > threshold else 0\n",
        "final_predictions = df_predictions.groupby('Case_ID').agg({\n",
        "    'Inclination_Prediction': lambda x: inclination_case_prediction(x, threshold=0.05),\n",
        "    'Inclination_Probability': 'mean'\n",
        "}).reset_index()\n",
        "final_predictions['Inclination_Defect'] = final_predictions['Inclination_Prediction'].map({0: 'No', 1: 'Yes'})\n",
        "results_table = final_predictions[['Case_ID', 'Inclination_Defect', 'Inclination_Probability']].copy()\n",
        "results_table['Inclination_Probability'] = results_table['Inclination_Probability'].round(3)\n",
        "timestamp_output_path = os.path.join(output_base_path, 'inclination_defect_timestamps.csv')\n",
        "inclination_timestamps_df.to_csv(timestamp_output_path, index=False)\n",
        "case_output_path = os.path.join(output_base_path, 'inclination_defect_cases.csv')\n",
        "results_table.to_csv(case_output_path, index=False)\n",
        "model_file = os.path.join(output_base_path, 'inclination_defect_model.keras')\n",
        "model.save(model_file)\n",
        "case_summary = inclination_timestamps_df.groupby('case_id').agg({\n",
        "    'Inclination_Prediction': 'sum',\n",
        "    'Time': 'count'\n",
        "}).rename(columns={'Time': 'Total_Timestamps'})\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
